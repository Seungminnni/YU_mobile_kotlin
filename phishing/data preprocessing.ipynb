{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ffa7c2-4adf-4e84-85f1-2c5e37f0cac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wja65\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================ 0) Imports ============================\n",
    "import os, re, gc, glob, math, zlib, base64, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ÏÑ†ÌÉùÏ†Å ÏùòÏ°¥ÏÑ±: Îçî Ï†ïÎ∞ÄÌïú HTML ÌååÏã±Ïö© (ÏóÜÏúºÎ©¥ Ï†ïÍ∑úÏãù Ìè¥Î∞±)\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except Exception:\n",
    "    BeautifulSoup = None\n",
    "\n",
    "# eTLD+1 Í≥ÑÏÇ∞(ÏûàÏúºÎ©¥ tldextract, ÏóÜÏúºÎ©¥ Í∞ÑÎã® Î≤ÑÏ†Ñ)\n",
    "try:\n",
    "    import tldextract\n",
    "except Exception:\n",
    "    tldextract = None\n",
    "\n",
    "def etld1(host: str) -> str:\n",
    "    host = (host or \"\").lower()\n",
    "    if not host: return \"\"\n",
    "    if tldextract:\n",
    "        ext = tldextract.extract(host)\n",
    "        return f\"{ext.domain}.{ext.suffix}\" if ext.suffix else ext.domain\n",
    "    parts = host.split(\".\")\n",
    "    return \".\".join(parts[-2:]) if len(parts) >= 2 else host\n",
    "\n",
    "# Ïù¥Ï†Ñ ÏÑ∏ÏÖò/Î™®Îç∏ Ï†ïÎ¶¨ (ÏãúÎìú Í≥†Ï†ï ÏóÜÏùå!)\n",
    "try: del model\n",
    "except: pass\n",
    "gc.collect()\n",
    "keras.backend.clear_session()\n",
    "\n",
    "\n",
    "# ============================ 1) Config =============================\n",
    "PHISHING_DIR = r\"C:/Users/wja65/phishing/phishing\"          # ÌîºÏã± Ìè¥Îçî(HTMLÎì§Ïù¥ Ï≠â)\n",
    "NORMAL_DIR   = r\"C:/Users/wja65/phishing/normal/normal\"      # Ï†ïÏÉÅ Ìè¥Îçî(HTMLÎì§Ïù¥ Ï≠â)\n",
    "\n",
    "# ‚òÖ Í∞úÏàò Ï†úÌïú (None Ïù¥Î©¥ Ï†ÑÎ∂Ä ÏÇ¨Ïö©)\n",
    "NORMAL_LIMIT   = None   # Ïòà: 6000, None\n",
    "PHISHING_LIMIT = None   # Ïòà: 1500, None\n",
    "\n",
    "# ÌïôÏäµ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
    "BATCH_SIZE, EPOCHS = 256, 60\n",
    "UNITS = [256, 128, 128, 64, 64, 32]\n",
    "DROPOUT, NOISE_STD, LR, WD, CLIPNORM = 0.30, 0.01, 3e-4, 1e-4, 1.0\n",
    "\n",
    "# Ï†ÄÏû• Í≤ΩÎ°ú\n",
    "SAVE_DIR = r\"C:/Users/wja65/phishing/outputs_html\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "SAVE_LAST = True\n",
    "LAST_SAVE_PATH = os.path.join(SAVE_DIR, \"last.csv\")\n",
    "PLOT_LOSS_PATH = os.path.join(SAVE_DIR, \"plot_loss.png\")\n",
    "PLOT_ACC_PATH  = os.path.join(SAVE_DIR, \"plot_acc.png\")\n",
    "PLOT_AUC_PATH  = os.path.join(SAVE_DIR, \"plot_auc.png\")\n",
    "REPORT_PATH    = os.path.join(SAVE_DIR, \"classification_report.txt\")\n",
    "CONFMAT_PATH   = os.path.join(SAVE_DIR, \"confusion_matrix.csv\")\n",
    "\n",
    "\n",
    "# ======================= 2) HTML ÌååÏùº ÌÉêÏßÄ(ÌôïÏû•Ïûê ÁÑ° ÎåÄÏùë) ==================\n",
    "HTML_EXTS = {\".html\", \".htm\", \".xhtml\", \".mhtml\", \".shtml\"}\n",
    "HTML_HEAD_RE = re.compile(r\"<!doctype\\s+html|<html\\b|<head\\b|<meta\\b|<title\\b|<body\\b\", re.IGNORECASE)\n",
    "\n",
    "def is_probably_html(path: str, sniff=8192) -> bool:\n",
    "    name = os.path.basename(path).lower()\n",
    "    _, ext = os.path.splitext(name)\n",
    "    if ext in HTML_EXTS:\n",
    "        return True\n",
    "    if name.endswith(\"_html\"):  # ÌôïÏû•Ïûê ÏóÜÏñ¥ÎèÑ `_html`Î°ú ÎÅùÎÇòÎ©¥ HTMLÎ°ú Í∞ÑÏ£º\n",
    "        return True\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            chunk = f.read(sniff)\n",
    "        txt = chunk.decode(\"utf-8\", errors=\"ignore\")\n",
    "        if HTML_HEAD_RE.search(txt):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def list_html_files(root_dir: str) -> list[str]:\n",
    "    root_dir = os.path.abspath(root_dir)\n",
    "    out = []\n",
    "    for r, _, files in os.walk(root_dir):\n",
    "        for fn in files:\n",
    "            fp = os.path.join(r, fn)\n",
    "            try:\n",
    "                if os.path.getsize(fp) <= 0:\n",
    "                    continue\n",
    "            except Exception:\n",
    "                continue\n",
    "            if is_probably_html(fp):\n",
    "                out.append(fp)\n",
    "    return out\n",
    "\n",
    "# ÏïàÏ†ÑÌïú ÌÖçÏä§Ìä∏ Î°úÎçî (ÌòºÌï© Ïù∏ÏΩîÎî© ÎåÄÏùë)\n",
    "def read_text_any(path: str) -> str | None:\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp949\", \"euc-kr\", \"latin-1\"):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n",
    "                return f.read()\n",
    "        except Exception:\n",
    "            continue\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return f.read()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ====================== 3) URL Ï∂îÏ†ï & ÌîºÏ≤ò Ìï®Ïàò ============================\n",
    "_URL_RE = re.compile(r\"https?://[^\\s\\\"')<>]+\", re.IGNORECASE)\n",
    "_CANON_RE = re.compile(r'rel=[\"\\']?canonical[\"\\']?.*?href=[\"\\']([^\"\\']+)[\"\\']', re.IGNORECASE|re.DOTALL)\n",
    "_OGURL_RE = re.compile(r'property=[\"\\']og:url[\"\\'].*?content=[\"\\']([^\"\\']+)[\"\\']', re.IGNORECASE|re.DOTALL)\n",
    "_BASE_RE = re.compile(r'<base[^>]*href=[\"\\']([^\"\\']+)[\"\\']', re.IGNORECASE)\n",
    "_META_REFRESH_URL_RE = re.compile(r'http-equiv=[\"\\']refresh[\"\\'].*?url=([^\"\\';>]+)', re.IGNORECASE)\n",
    "\n",
    "def guess_url_from_html(text: str, fallback_path: str) -> str:\n",
    "    html = text or \"\"\n",
    "    for regex in (_CANON_RE, _OGURL_RE, _BASE_RE, _META_REFRESH_URL_RE):\n",
    "        m = regex.search(html)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "    m = _URL_RE.search(html)\n",
    "    if m:\n",
    "        return m.group(0).strip()\n",
    "    return \"file://\" + fallback_path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "def shannon_entropy(s: str) -> float:\n",
    "    if not s: return 0.0\n",
    "    counts = Counter(s)\n",
    "    n = len(s)\n",
    "    return -sum((c/n) * math.log2(c/n) for c in counts.values())\n",
    "\n",
    "JS_REDIRECT_SIGNS = [\n",
    "    \"location.href\", \"location.replace\", \"window.location\", \"top.location\",\n",
    "    \"document.location\", \"self.location\", \"location.assign\", \"meta http-equiv=\\\"refresh\\\"\"\n",
    "]\n",
    "OBFUSCATION_SIGNS = [\n",
    "    \"atob(\", \"btoa(\", \"fromCharCode\", \"unescape(\", \"eval(\", \"Function(\", \"setTimeout(\",\n",
    "    \"String.fromCharCode\", \"document.write(\", \"obfuscate\", \"charCodeAt\"\n",
    "]\n",
    "SUS_KEYWORDS = [\n",
    "    \"login\",\"sign in\",\"signin\",\"verify\",\"password\",\"passcode\",\"otp\",\n",
    "    \"bank\",\"account\",\"billing\",\"security\",\"update\",\"restricted\",\"unlock\",\n",
    "    \"coin\",\"wallet\",\"metamask\",\"seed\",\"mnemonic\"\n",
    "]\n",
    "\n",
    "def extract_urls_bs4(soup):\n",
    "    urls = []\n",
    "    for tag in soup.find_all([\"a\",\"link\",\"img\",\"script\",\"iframe\",\"source\",\"video\",\"audio\",\"form\"]):\n",
    "        for attr in [\"href\",\"src\",\"action\",\"data-src\"]:\n",
    "            u = tag.get(attr)\n",
    "            if not u: continue\n",
    "            if isinstance(u, list): \n",
    "                for x in u:\n",
    "                    if isinstance(x, str) and x.startswith((\"http://\",\"https://\")):\n",
    "                        urls.append(x)\n",
    "            elif isinstance(u, str) and u.startswith((\"http://\",\"https://\")):\n",
    "                urls.append(u)\n",
    "    return urls\n",
    "\n",
    "def extract_all_http_urls(text: str) -> list[str]:\n",
    "    if BeautifulSoup:\n",
    "        try:\n",
    "            soup = BeautifulSoup(text, \"html.parser\")\n",
    "            urls = extract_urls_bs4(soup)\n",
    "            # Ï†ïÍ∑úÏãù Î≥¥Ï°∞\n",
    "            urls += _URL_RE.findall(text)\n",
    "            return list({u for u in urls if u.startswith((\"http://\",\"https://\"))})\n",
    "        except Exception:\n",
    "            pass\n",
    "    return list({u for u in _URL_RE.findall(text) if u.startswith((\"http://\",\"https://\"))})\n",
    "\n",
    "def url_lex(url: str) -> dict:\n",
    "    s = str(url or \"\")\n",
    "    p = urlparse(s)\n",
    "    host = p.hostname or \"\"\n",
    "    digits = sum(ch.isdigit() for ch in s)\n",
    "    return {\n",
    "        \"lex_len_url\": len(s),\n",
    "        \"lex_len_host\": len(host),\n",
    "        \"lex_nb_dot\": s.count(\".\"),\n",
    "        \"lex_nb_dash\": s.count(\"-\"),\n",
    "        \"lex_nb_slash\": s.count(\"/\"),\n",
    "        \"lex_nb_qm\": s.count(\"?\"),\n",
    "        \"lex_nb_eq\": s.count(\"=\"),\n",
    "        \"lex_ratio_digits\": digits / max(1, len(s)),\n",
    "        \"lex_nb_subdomains\": max(0, host.count(\".\") - 1),\n",
    "        \"lex_has_ip_host\": int(bool(re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", host))),\n",
    "        \"lex_path_len\": len(p.path or \"\"),\n",
    "        \"lex_query_len\": len(p.query or \"\"),\n",
    "        \"lex_port_nonstd\": int(p.port not in [80, 443, None]),\n",
    "    }\n",
    "\n",
    "def extract_html_features(text: str) -> dict:\n",
    "    if not text: text = \"\"\n",
    "    low = text.lower()\n",
    "    feats = {\n",
    "        # Í∏∏Ïù¥/ÎπÑÏú®/ÏóîÌä∏Î°úÌîº\n",
    "        \"html_len\": len(text),\n",
    "        \"html_nonascii_ratio\": sum(ord(c)>127 for c in text)/max(1,len(text)),\n",
    "        \"html_percent_encoded_ratio\": low.count(\"%\")/max(1,len(text)),\n",
    "        \"html_entropy\": shannon_entropy(text[:20000]),  # ÎÑàÎ¨¥ Í∏∏Î©¥ ÌÅ¥Î¶Ω\n",
    "\n",
    "        # ÌÉúÍ∑∏/Ìèº/ÏûÖÎ†•\n",
    "        \"html_nb_script\": 0, \"html_nb_iframe\": 0, \"html_nb_form\": 0,\n",
    "        \"html_nb_input\": 0, \"html_nb_pw_input\": 0, \"html_nb_link\": 0,\n",
    "        \"html_nb_button\": 0,\n",
    "\n",
    "        # Ïù¥Î≤§Ìä∏ Ìï∏Îì§Îü¨/ÎÇúÎèÖÌôî/Îç∞Ïù¥ÌÑ∞ URI\n",
    "        \"html_has_onclick\": int(\"onclick=\" in low),\n",
    "        \"html_has_data_uri\": int(\"data:image\" in low),\n",
    "        \"html_has_eval\": int(\"eval(\" in low),\n",
    "        \"html_nb_obfuscation_kw\": sum(1 for sig in OBFUSCATION_SIGNS if sig.lower() in low),\n",
    "\n",
    "        # ÏùòÏã¨ ÌÇ§ÏõåÎìú\n",
    "        \"html_nb_sus_kw\": sum(low.count(kw) for kw in SUS_KEYWORDS),\n",
    "\n",
    "        # JS Î¶¨Îã§Ïù¥Î†âÏÖò ÏãúÍ∑∏ÎÑê\n",
    "        \"html_nb_js_redirect\": sum(low.count(sig) for sig in JS_REDIRECT_SIGNS),\n",
    "    }\n",
    "\n",
    "    if BeautifulSoup:\n",
    "        try:\n",
    "            soup = BeautifulSoup(text, \"html.parser\")\n",
    "            feats[\"html_nb_script\"]  = len(soup.find_all(\"script\"))\n",
    "            feats[\"html_nb_iframe\"]  = len(soup.find_all(\"iframe\"))\n",
    "            feats[\"html_nb_form\"]    = len(soup.find_all(\"form\"))\n",
    "            inputs = soup.find_all(\"input\")\n",
    "            feats[\"html_nb_input\"]   = len(inputs)\n",
    "            feats[\"html_nb_pw_input\"]= sum(1 for i in inputs if (i.get(\"type\") or \"\").lower()==\"password\")\n",
    "            feats[\"html_nb_link\"]    = len(soup.find_all(\"a\"))\n",
    "            feats[\"html_nb_button\"]  = len(soup.find_all(\"button\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        feats[\"html_nb_script\"]   = len(re.findall(r\"<script\\b\", text, re.IGNORECASE))\n",
    "        feats[\"html_nb_iframe\"]   = len(re.findall(r\"<iframe\\b\", text, re.IGNORECASE))\n",
    "        feats[\"html_nb_form\"]     = len(re.findall(r\"<form\\b\", text, re.IGNORECASE))\n",
    "        feats[\"html_nb_input\"]    = len(re.findall(r\"<input\\b\", text, re.IGNORECASE))\n",
    "        feats[\"html_nb_pw_input\"] = len(re.findall(r'<input[^>]*type=[\"\\']password', text, re.IGNORECASE))\n",
    "        feats[\"html_nb_link\"]     = len(re.findall(r\"<a\\b\", text, re.IGNORECASE))\n",
    "        feats[\"html_nb_button\"]   = len(re.findall(r\"<button\\b\", text, re.IGNORECASE))\n",
    "    return feats\n",
    "\n",
    "def external_link_features(main_url: str, urls: list[str]) -> dict:\n",
    "    # Î©îÏù∏ ÎèÑÎ©îÏù∏ Ï∂îÏ†ï\n",
    "    p = urlparse(main_url)\n",
    "    main_host = p.hostname or \"\"\n",
    "    main_e = etld1(main_host) if main_host else \"\"\n",
    "\n",
    "    ehosts = [etld1(urlparse(u).hostname or \"\") for u in urls if (u and urlparse(u).scheme in (\"http\",\"https\"))]\n",
    "    ehosts = [e for e in ehosts if e]  # ÎπàÍ∞í Ï†úÍ±∞\n",
    "    unique_ext = {e for e in ehosts if e != main_e and e != \"\"}\n",
    "    return {\n",
    "        \"link_total\": len(urls),\n",
    "        \"link_unique_domain\": len(set(ehosts)),\n",
    "        \"link_external_domain\": len(unique_ext),\n",
    "        \"link_external_ratio\": (len(unique_ext)/max(1,len(set(ehosts)))),\n",
    "    }\n",
    "\n",
    "def redirection_heuristics(text: str) -> dict:\n",
    "    low = (text or \"\").lower()\n",
    "    patterns = [\n",
    "        r'\\bwindow\\.location\\s*=\\s*',\n",
    "        r'\\blocation\\.href\\s*=\\s*',\n",
    "        r'\\bmeta\\s+http-equiv=[\"\\']refresh',\n",
    "        r'\\bsettimeout\\s*\\(',\n",
    "        r'\\btop\\.location\\s*=\\s*',\n",
    "    ]\n",
    "    return {\"redir_hits\": sum(len(re.findall(p, low)) for p in patterns)}\n",
    "\n",
    "\n",
    "# ======================= 4) Ìè¥Îçî Î°úÎìú & ÎùºÎ≤®ÎßÅ ============================\n",
    "def load_html_dir(dir_path: str, label: int, label_name: str, limit: int|None) -> pd.DataFrame:\n",
    "    files = list_html_files(dir_path)\n",
    "    if not files:\n",
    "        print(f\"[warn] HTML-like ÌååÏùº ÏóÜÏùå: {dir_path}\")\n",
    "        return pd.DataFrame(columns=[\"filepath\",\"url\",\"label\",\"label_name\",\"html\"])\n",
    "    idx = np.arange(len(files))\n",
    "    np.random.shuffle(idx)         # ÏãúÎìú ÏóÜÏùå\n",
    "    if limit is not None:\n",
    "        idx = idx[:min(limit, len(idx))]\n",
    "    chosen = [files[i] for i in idx]\n",
    "\n",
    "    rows = []\n",
    "    for fp in chosen:\n",
    "        text = read_text_any(fp)\n",
    "        if not text:\n",
    "            continue\n",
    "        url = guess_url_from_html(text, fallback_path=fp)\n",
    "        rows.append({\"filepath\": fp, \"url\": url, \"label\": label, \"label_name\": label_name, \"html\": text})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_normal  = load_html_dir(NORMAL_DIR,   label=0, label_name=\"normal\",   limit=NORMAL_LIMIT)\n",
    "df_phish   = load_html_dir(PHISHING_DIR, label=1, label_name=\"phishing\", limit=PHISHING_LIMIT)\n",
    "\n",
    "last = pd.concat([df_normal, df_phish], ignore_index=True).sample(frac=1.0).reset_index(drop=True)\n",
    "\n",
    "print(\"Counts after limiting:\")\n",
    "print(\"  normal:\", len(df_normal), \"| phishing:\", len(df_phish))\n",
    "print(\"last shape:\", last.shape)\n",
    "print(\"class value_counts:\\n\", last[\"label\"].value_counts())\n",
    "\n",
    "if last[\"label\"].nunique() < 2:\n",
    "    raise SystemExit(\"‚ùå Îëê ÌÅ¥ÎûòÏä§(0/1)Í∞Ä Î™®Îëê ÌïÑÏöîÌï©ÎãàÎã§. Ìè¥Îçî/Ï†úÌïú ÏÑ§Ï†ï ÌôïÏù∏.\")\n",
    "\n",
    "if SAVE_LAST:\n",
    "    last[[\"filepath\",\"url\",\"label\",\"label_name\"]].to_csv(LAST_SAVE_PATH, index=False, encoding=\"utf-8\")\n",
    "    print(\"‚úÖ last saved to:\", LAST_SAVE_PATH)\n",
    "\n",
    "\n",
    "# ============================ 5) Feature Set =========================\n",
    "# 5-1) URL lexical (ÎåÄÌëú URL Í∏∞Ï§Ä)\n",
    "lex = last[\"url\"].map(url_lex).apply(pd.Series)\n",
    "\n",
    "# 5-2) HTML Íµ¨Ï°∞ & ÎÇúÎèÖÌôî/Î¶¨Îã§Ïù¥Î†âÏÖò ÏßÄÌëú\n",
    "html_feats = last[\"html\"].map(extract_html_features).apply(pd.Series)\n",
    "\n",
    "# 5-3) Î¨∏ÏÑú ÎÇ¥ Î™®Îì† Ïô∏Î∂Ä ÎßÅÌÅ¨ Î∂ÑÏÑù\n",
    "all_urls = last[\"html\"].map(extract_all_http_urls)\n",
    "ext_link_feats = []\n",
    "for main_u, urls in zip(last[\"url\"], all_urls):\n",
    "    f = external_link_features(main_u, urls)\n",
    "    ext_link_feats.append(f)\n",
    "ext_link_feats = pd.DataFrame(ext_link_feats)\n",
    "\n",
    "# 5-4) Ï∂îÍ∞Ä Î¶¨Îã§Ïù¥Î†âÏÖò ÏãúÍ∑∏ÎÑê\n",
    "redir_feats = last[\"html\"].map(redirection_heuristics).apply(pd.Series)\n",
    "\n",
    "# Ìï©ÏπòÍ∏∞\n",
    "last_feats = pd.concat(\n",
    "    [last[[\"url\",\"label\",\"label_name\",\"filepath\"]],\n",
    "     lex, html_feats, ext_link_feats, redir_feats],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "feature_cols = [c for c in last_feats.columns\n",
    "                if c.startswith((\"lex_\",\"html_\",\"link_\",\"redir_\"))]\n",
    "X_all = last_feats[feature_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0).astype(\"float32\").to_numpy()\n",
    "y_all = last_feats[\"label\"].astype(\"float32\").to_numpy()\n",
    "\n",
    "print(\"n_features:\", len(feature_cols))\n",
    "\n",
    "\n",
    "# ====== 6) Split (ÎèÑÎ©îÏù∏/Ìè¥Îçî Í∏∞Î∞ò Í∑∏Î£πÏúºÎ°ú ÎàÑÏàò Î∞©ÏßÄ: ÎèôÏùºÍ∑∏Î£πÏùÄ ÌïúÏ™ΩÏóêÎßå) ======\n",
    "def group_from(u: str, fp: str) -> str:\n",
    "    host = urlparse(str(u)).hostname\n",
    "    if host:\n",
    "        return etld1(host)\n",
    "    # file:// ÏºÄÏù¥Ïä§ ‚Üí ÏÉÅÏúÑ Ìè¥ÎçîÎ™ÖÏúºÎ°ú Í∑∏Î£π\n",
    "    return os.path.basename(os.path.dirname(fp)) or \"nogroup\"\n",
    "\n",
    "groups = pd.Series([group_from(u, fp) for u, fp in zip(last_feats[\"url\"], last_feats[\"filepath\"])],\n",
    "                   index=last_feats.index)\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8)  # ÏãúÎìú ÏßÄÏ†ï Ïïà Ìï®\n",
    "tr_i, va_i = next(gss.split(X_all, y_all, groups=groups))\n",
    "\n",
    "X_train, y_train = X_all[tr_i], y_all[tr_i]\n",
    "X_val,   y_val   = X_all[va_i], y_all[va_i]\n",
    "print(\"Split ->\", X_train.shape, X_val.shape)\n",
    "\n",
    "# ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò\n",
    "classes = np.unique(y_train.astype(int))\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train.astype(int))\n",
    "class_weight = {int(k): float(v) for k, v in zip(classes, cw)}\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "\n",
    "# ============================ 7) Model =====================================\n",
    "def build_res_mlp(input_dim: int, X_train_sample: np.ndarray) -> keras.Model:\n",
    "    inp  = keras.Input(shape=(input_dim,), name=\"features\")\n",
    "    norm = layers.Normalization(name=\"norm\")\n",
    "    norm.adapt(X_train_sample)  # train ÌÜµÍ≥ÑÎßå ÏÇ¨Ïö©\n",
    "\n",
    "    x = norm(inp)\n",
    "    x = layers.GaussianNoise(NOISE_STD)(x)\n",
    "    x = layers.Dropout(DROPOUT)(x)\n",
    "\n",
    "    def block(x, units):\n",
    "        h = layers.Dense(units, activation=\"relu\",\n",
    "                         kernel_regularizer=keras.regularizers.L2(WD))(x)\n",
    "        h = layers.Dropout(DROPOUT)(h)\n",
    "        h = layers.Dense(units, activation=\"relu\",\n",
    "                         kernel_regularizer=keras.regularizers.L2(WD))(h)\n",
    "        if x.shape[-1] != units:\n",
    "            x = layers.Dense(units, activation=None)(x)\n",
    "        return layers.Add()([x, h])\n",
    "\n",
    "    x = layers.Dense(UNITS[0], activation=\"relu\")(x)\n",
    "    for u in UNITS[1:]:\n",
    "        x = block(x, u)\n",
    "\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inp, out, name=\"phish_resmlp\")\n",
    "\n",
    "    opt  = keras.optimizers.Adam(learning_rate=LR, clipnorm=CLIPNORM)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "                 keras.metrics.AUC(name=\"auc\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_res_mlp(X_train.shape[1], X_train)\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=6,\n",
    "                                  restore_best_weights=True, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\",\n",
    "                                      factor=0.5, patience=3, verbose=1),\n",
    "]\n",
    "\n",
    "\n",
    "# ============================ 8) Train =====================================\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks, verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# ============================ 9) Plot & Eval ===============================\n",
    "def save_hist_plot(hist, keys, title, out_path, ylim=None, ylabel=None):\n",
    "    plt.figure()\n",
    "    plotted = False\n",
    "    for k in keys:\n",
    "        if k in hist.history:\n",
    "            plt.plot(hist.history[k], label=k)\n",
    "            plotted = True\n",
    "    if not plotted:\n",
    "        print(f\"[warn] {keys} ÏóÜÏùå, ÌîåÎ°Ø ÏÉùÎûµ\")\n",
    "        return\n",
    "    plt.title(title); plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(ylabel if ylabel else keys[0])\n",
    "    if ylim: plt.ylim(*ylim)\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"üìà Saved plot -> {out_path}\")\n",
    "\n",
    "save_hist_plot(history, [\"loss\",\"val_loss\"], \"Loss\", PLOT_LOSS_PATH, ylabel=\"Loss\")\n",
    "save_hist_plot(history, [\"accuracy\",\"val_accuracy\"], \"Accuracy\", PLOT_ACC_PATH, ylim=(0,1), ylabel=\"Accuracy\")\n",
    "save_hist_plot(history, [\"auc\",\"val_auc\"], \"ROC-AUC\", PLOT_AUC_PATH, ylim=(0,1), ylabel=\"AUC\")\n",
    "\n",
    "# Validation ÏÑ±Îä• ÏàòÏπò\n",
    "val_prob = model.predict(X_val, verbose=0).ravel()\n",
    "val_pred = (val_prob >= 0.5).astype(int)\n",
    "\n",
    "target_names = [\"normal(0)\", \"phishing(1)\"]\n",
    "report = classification_report(y_val.astype(int), val_pred, target_names=target_names, digits=4)\n",
    "cm = confusion_matrix(y_val.astype(int), val_pred, labels=[0,1])\n",
    "try:\n",
    "    auc_val = roc_auc_score(y_val, val_prob)\n",
    "except Exception:\n",
    "    auc_val = float(\"nan\")\n",
    "\n",
    "print(\"\\n=== Validation Report ===\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix (rows=true, cols=pred):\\n\", cm)\n",
    "print(f\"Val ROC-AUC: {auc_val:.4f}\")\n",
    "\n",
    "# Ï†ÄÏû•\n",
    "with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Validation Report ===\\n\")\n",
    "    f.write(report + \"\\n\\n\")\n",
    "    f.write(\"Confusion Matrix (rows=true, cols=pred):\\n\")\n",
    "    f.write(pd.DataFrame(cm, index=[\"true_normal\",\"true_phishing\"], columns=[\"pred_normal\",\"pred_phishing\"]).to_string() + \"\\n\\n\")\n",
    "    f.write(f\"Val ROC-AUC: {auc_val:.6f}\\n\")\n",
    "print(f\"üìù Saved report -> {REPORT_PATH}\")\n",
    "\n",
    "pd.DataFrame(cm, index=[\"true_normal\",\"true_phishing\"], columns=[\"pred_normal\",\"pred_phishing\"])\\\n",
    "  .to_csv(CONFMAT_PATH, encoding=\"utf-8\", index=True)\n",
    "print(f\"üßæ Saved confusion matrix -> {CONFMAT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
