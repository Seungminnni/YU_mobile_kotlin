{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cd8263d4-5059-4d8d-b6a8-3f66a35bf38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Reset & Imports =====\n",
    "# 환경 설정: tf_env 사용\n",
    "import os\n",
    "os.environ['CONDA_DEFAULT_ENV'] = 'tf_env'\n",
    "\n",
    "import gc, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b90520d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 경로: /opt/anaconda3/envs/tf_env/bin/python\n",
      "Python 버전: 3.11.11 | packaged by conda-forge | (main, Dec  5 2024, 08:47:03) [Clang 18.1.8 ]\n",
      "TensorFlow 버전: 2.18.0\n",
      "\n",
      "설치된 패키지 일부:\n",
      "\n",
      "설치된 패키지 일부:\n"
     ]
    }
   ],
   "source": [
    "# Python 환경 확인\n",
    "import sys\n",
    "print(\"Python 경로:\", sys.executable)\n",
    "print(\"Python 버전:\", sys.version)\n",
    "\n",
    "# 설치된 패키지 확인\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(\"TensorFlow 버전:\", tf.__version__)\n",
    "except ImportError as e:\n",
    "    print(\"TensorFlow 임포트 실패:\", e)\n",
    "    \n",
    "# pip 경로 확인\n",
    "import subprocess\n",
    "result = subprocess.run([sys.executable, '-m', 'pip', 'list'], capture_output=True, text=True)\n",
    "print(\"\\n설치된 패키지 일부:\")\n",
    "for line in result.stdout.split('\\n')[:10]:\n",
    "    if 'tensor' in line.lower():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a032a0ab-7fd3-4252-882f-02d1412a8ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전 초기화\n",
    "try: del model\n",
    "except: pass\n",
    "gc.collect()\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42); tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d2755837-2f19-4330-9385-36cf47073f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1) Config =====\n",
    "# Use the repo-local TFLite-ready CSV so feature names/order match mobile assets\n",
    "# Use absolute paths so notebook runs reliably regardless of working directory\n",
    "# Set these to your repo path — change if your project is elsewhere on disk\n",
    "DATA_PATH = \"/Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/phishing_data_tflite_ready.csv\"\n",
    "TARGET = \"status\"\n",
    "BATCH_SIZE, EPOCHS = 32, 50 #배치 에폭 \n",
    "# LABEL_MODE selects training target: 'soft' uses probability labels, 'binary' uses 0/1, 'both' keeps both and defaults to soft training\n",
    "LABEL_MODE = 'both'  # options: 'soft' | 'binary' | 'both'\n",
    "# APPLY_SAMPLE_WEIGHT: when True we'll down-weight samples with labels near 0.5 (uncertain).\n",
    "# If you want to preserve 0.5 exactly and not reduce its influence, set this to False\n",
    "APPLY_SAMPLE_WEIGHT = False #필요에 따라 수정\n",
    "# EARLY_STOP_AT_ACC = 0.99\n",
    "# LABEL_NA_POLICY controls how rows with missing/invalid label_soft are handled\n",
    "# options: 'drop' (remove rows), 'impute' (fill with median), 'raise' (error out)\n",
    "LABEL_NA_POLICY = 'drop'\n",
    "# If using 'impute', this value will be used (median recommended)\n",
    "LABEL_NA_IMPUTE_VALUE = None\n",
    "# ===== 2) Load & Clean =====\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "if \"url\" in df.columns: df = df.drop(columns=[\"url\"])\n",
    "df = shuffle(df, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "68322c57-07f9-4074-8e65-287c1a7e6f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_soft NaNs: 0\n",
      "LABEL_NA_POLICY = drop\n",
      "label_soft min/max/mean: 0.0 1.0 0.5000436\n",
      "Label counts (soft approx) top: {1.0: 5741, 0.0: 5740}\n",
      "Label counts (binary): {1.0: 5741, 0.0: 5740}\n"
     ]
    }
   ],
   "source": [
    "# 라벨 정리 (soft-first 안전 처리) — soft 라벨을 우선 보존합니다\n",
    "# 1) 표준화: 문자열 정리 및 명시적 매핑\n",
    "tmp = df[TARGET].astype(str).str.strip().str.lower()\n",
    "mapping = {'legitimate': '0', 'phishing': '1', 'zero': '0', 'one': '1'}\n",
    "tmp = tmp.replace(mapping)\n",
    "# 2) 숫자로 안전하게 변환 — invalid는 NaN으로 처리\n",
    "df['label_soft'] = pd.to_numeric(tmp, errors='coerce').astype('float32')\n",
    "# 3) NaN / 범위 검사\n",
    "n_missing = int(df['label_soft'].isna().sum())\n",
    "print('label_soft NaNs:', n_missing)\n",
    "if 'LABEL_NA_POLICY' in globals():\n",
    "    print('LABEL_NA_POLICY =', LABEL_NA_POLICY)\n",
    "if n_missing > 0:\n",
    "    if LABEL_NA_POLICY == 'drop':\n",
    "        print(f'Dropping {n_missing} rows with missing labels')\n",
    "        df = df.dropna(subset=['label_soft']).reset_index(drop=True)\n",
    "    elif LABEL_NA_POLICY == 'impute':\n",
    "        if LABEL_NA_IMPUTE_VALUE is None:\n",
    "            fill = float(df['label_soft'].median())\n",
    "        else:\n",
    "            fill = float(LABEL_NA_IMPUTE_VALUE)\n",
    "        print('Imputing missing labels with', fill)\n",
    "        df['label_soft'] = df['label_soft'].fillna(fill)\n",
    "    elif LABEL_NA_POLICY == 'raise':\n",
    "        raise ValueError('Found missing labels (label_soft) and LABEL_NA_POLICY==raise')\n",
    "# 4) Ensure label range is [0,1] — clip with warning if out of bounds\n",
    "minv, maxv = df['label_soft'].min(), df['label_soft'].max()\n",
    "if minv < 0 or maxv > 1:\n",
    "    print('Warning: label_soft out of [0,1] range — clipping')\n",
    "    df['label_soft'] = df['label_soft'].clip(0.0, 1.0)\n",
    "print('label_soft min/max/mean:', df['label_soft'].min(), df['label_soft'].max(), df['label_soft'].mean())\n",
    "# 5) hard binary label 생성 (앱/배포 호환성 용도)\n",
    "df['label_binary'] = (df['label_soft'] >= 0.5).astype('float32')\n",
    "print('Label counts (soft approx) top:', df['label_soft'].value_counts().head(10).to_dict())\n",
    "print('Label counts (binary):', df['label_binary'].value_counts().to_dict())\n",
    "# Note: we keep both label_soft and label_binary in the dataframe; training target selection controlled by LABEL_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7006c5fe-98b5-4ecb-8c0a-228c61b6cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3) 숫자 피처만 안전하게 선택 =====\n",
    "# Exclude the target and any internal label columns we created earlier (label_soft, label_binary)\n",
    "exclude_cols = {TARGET, 'label_soft', 'label_binary'}\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "numeric_cols = []\n",
    "for c in feature_cols:\n",
    "    ser = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if ser.notna().any():           # 전부 NaN이 아니면 사용\n",
    "        df[c] = ser\n",
    "        numeric_cols.append(c)\n",
    "\n",
    "assert len(numeric_cols) > 0, \"사용 가능한 숫자 피처가 없습니다.\"\n",
    "# # --- Debugging output: show exactly which feature columns were candidates and which numeric columns were selected\n",
    "# print('\\n=== Feature selection summary ===')\n",
    "# print('total df columns:', len(df.columns))\n",
    "# # show excluded columns, and whether they are present in the current dataframe\n",
    "# print('excluded columns (target + internal labels):', sorted(list(exclude_cols)))\n",
    "# present_excluded = [c for c in exclude_cols if c in df.columns]\n",
    "# missing_excluded = [c for c in exclude_cols if c not in df.columns]\n",
    "# print('excluded_present_in_df:', present_excluded)\n",
    "# print('excluded_missing_in_df:', missing_excluded)\n",
    "# print('feature candidate columns (feature_cols) count:', len(feature_cols))\n",
    "# print('feature candidate preview (first 20):', feature_cols[:20])\n",
    "# print('numeric columns selected (numeric_cols) count:', len(numeric_cols))\n",
    "# print('numeric columns selected:', numeric_cols)\n",
    "# label_in_numeric = [c for c in numeric_cols if c in (TARGET, 'label_soft', 'label_binary')]\n",
    "# if label_in_numeric:\n",
    "#     print('WARNING: numeric_cols unexpectedly contains label-like columns:', label_in_numeric)\n",
    "# else:\n",
    "#     print('OK: no internal label columns are present in numeric_cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cbc18bd6-4637-4da0-8aa1-a8ec0a6b12a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (9184, 79)  | X_val: (2297, 79)  | n_features: 79\n"
     ]
    }
   ],
   "source": [
    "# 결측치 중앙값으로 채우기\n",
    "for c in numeric_cols:\n",
    "    med = float(df[c].dropna().median()) if df[c].notna().any() else 0.0\n",
    "    df[c] = df[c].fillna(med).astype(\"float32\")\n",
    "\n",
    "# ===== 4) 행렬(X, y)로 변환 =====\n",
    "X = df[numeric_cols].to_numpy(dtype=\"float32\")\n",
    "# Prepare soft and binary targets — do not overwrite original soft values\n",
    "y_soft = df['label_soft'].to_numpy(dtype=\"float32\")\n",
    "y_binary = df['label_binary'].to_numpy(dtype=\"float32\")\n",
    "# Choose training target based on LABEL_MODE (soft | binary | both). 'both' will train using soft by default\n",
    "if LABEL_MODE == 'soft':\n",
    "    y = y_soft\n",
    "elif LABEL_MODE == 'binary':\n",
    "    y = y_binary\n",
    "else:  # 'both' default to soft for training but keep binary for evaluation/export\n",
    "    y = y_soft\n",
    "\n",
    "\n",
    "# Use index-based split so soft and binary targets align with the same train/validation indices\n",
    "# Ensure no NaNs in the stratify label (y_binary). If any appear, drop those rows first\n",
    "mask_valid = ~np.isnan(y_binary)\n",
    "if not mask_valid.all():\n",
    "    n_bad = int((~mask_valid).sum())\n",
    "    print(f'Warning: {n_bad} rows with NaN binary label found. Dropping before split.')\n",
    "    X = X[mask_valid]\n",
    "    y_soft = y_soft[mask_valid]\n",
    "    y_binary = y_binary[mask_valid]\n",
    "indices = np.arange(X.shape[0])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=y_binary, random_state=42)\n",
    "X_train, X_val = X[train_idx], X[val_idx]\n",
    "# aligned splits for both target types\n",
    "y_soft_train, y_soft_val = y_soft[train_idx], y_soft[val_idx]\n",
    "y_binary_train, y_binary_val = y_binary[train_idx], y_binary[val_idx]\n",
    "if LABEL_MODE == 'soft':\n",
    "    y_train, y_val = y_soft_train, y_soft_val\n",
    "elif LABEL_MODE == 'binary':\n",
    "    y_train, y_val = y_binary_train, y_binary_val\n",
    "else:  # both - default to soft training, keep binary for evaluation\n",
    "    y_train, y_val = y_soft_train, y_soft_val\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \" | X_val:\", X_val.shape, \" | n_features:\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7beee866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train checks — shapes & metadata\n",
      "X_train shape: (9184, 79)\n",
      "X_val shape: (2297, 79)\n",
      "n_features (numeric_cols): 79\n",
      "Found ../app/src/main/assets/feature_info.json: input_shape=[79], feature_columns=79\n",
      "y_soft_train min/max/mean: 0.0 1.0 0.5\n",
      "y_binary_train counts: (array([0., 1.], dtype=float32), array([4592, 4592]))\n",
      "Pre-train checks done — proceed to model creation if OK\n"
     ]
    }
   ],
   "source": [
    "# ===== PRE-TRAIN CHECKS =====\n",
    "import os, json, numpy as np\n",
    "print('Pre-train checks — shapes & metadata')\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('n_features (numeric_cols):', len(numeric_cols))\n",
    "if X_train.shape[1] != len(numeric_cols):\n",
    "    raise AssertionError(f'ERROR: feature column count mismatch: X_train has {X_train.shape[1]} cols but numeric_cols lists {len(numeric_cols)}')\n",
    "# Compare to existing feature_info in repo/app assets if present\n",
    "fi_paths = ['phishing/feature_info.json', 'app/src/main/assets/feature_info.json', '../app/src/main/assets/feature_info.json']\n",
    "for p in fi_paths:\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            with open(p) as f:\n",
    "                fi = json.load(f)\n",
    "            input_shape = fi.get('input_shape')\n",
    "            feat_cols = fi.get('feature_columns', [])\n",
    "            print(f'Found {p}: input_shape={input_shape}, feature_columns={len(feat_cols)}')\n",
    "            if isinstance(input_shape, (list, tuple)) and len(input_shape) > 0 and input_shape[0] != X_train.shape[1]:\n",
    "                print(f'WARNING: {p} input_shape[0] ({input_shape[0]}) != n_features ({X_train.shape[1]})')\n",
    "        except Exception as e:\n",
    "            print('Could not read', p, e)\n",
    "# basic label distribution checks\n",
    "print('y_soft_train min/max/mean:', np.min(y_soft_train), np.max(y_soft_train), np.mean(y_soft_train))\n",
    "print('y_binary_train counts:', np.unique(y_binary_train, return_counts=True))\n",
    "# sample_weight sanity if enabled (applies when training on soft labels)\n",
    "if APPLY_SAMPLE_WEIGHT and LABEL_MODE in ('soft', 'both'):\n",
    "    sw = np.clip(np.abs(y_train - 0.5) * 2.0, 0.1, 1.0)\n",
    "    print('sample_weight min/max/mean:', sw.min(), sw.max(), sw.mean())\n",
    "print('Pre-train checks done — proceed to model creation if OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "efbeb252-e2ed-4e31-b0d7-0feb6df67368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"phish_numeric_only\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"phish_numeric_only\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ features (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ norm_all (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">159</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ features (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ norm_all (\u001b[38;5;33mNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m)             │           \u001b[38;5;34m159\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m5,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,392</span> (28.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,392\u001b[0m (28.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,233</span> (28.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,233\u001b[0m (28.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">159</span> (640.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m159\u001b[0m (640.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== 5) 모델 (단일 입력) =====\n",
    "inp = keras.Input(shape=(X_train.shape[1],), name=\"features\", dtype=tf.float32)\n",
    "norm = layers.Normalization(name=\"norm_all\")\n",
    "norm.adapt(X_train)\n",
    "\n",
    "x = norm(inp)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inp, outputs=out, name=\"phish_numeric_only\")\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0992483d-0794-444d-9f7d-b45ad29ba174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729us/step - accuracy: 0.8371 - loss: 0.3676 - val_accuracy: 0.8964 - val_loss: 0.2557\n",
      "Epoch 2/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729us/step - accuracy: 0.8371 - loss: 0.3676 - val_accuracy: 0.8964 - val_loss: 0.2557\n",
      "Epoch 2/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - accuracy: 0.9076 - loss: 0.2372 - val_accuracy: 0.9108 - val_loss: 0.2250\n",
      "Epoch 3/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - accuracy: 0.9076 - loss: 0.2372 - val_accuracy: 0.9108 - val_loss: 0.2250\n",
      "Epoch 3/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - accuracy: 0.9194 - loss: 0.2094 - val_accuracy: 0.9177 - val_loss: 0.2109\n",
      "Epoch 4/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - accuracy: 0.9194 - loss: 0.2094 - val_accuracy: 0.9177 - val_loss: 0.2109\n",
      "Epoch 4/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.9263 - loss: 0.1938 - val_accuracy: 0.9269 - val_loss: 0.1998\n",
      "Epoch 5/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.9263 - loss: 0.1938 - val_accuracy: 0.9269 - val_loss: 0.1998\n",
      "Epoch 5/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.9301 - loss: 0.1832 - val_accuracy: 0.9308 - val_loss: 0.1907\n",
      "Epoch 6/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.9301 - loss: 0.1832 - val_accuracy: 0.9308 - val_loss: 0.1907\n",
      "Epoch 6/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - accuracy: 0.9356 - loss: 0.1702 - val_accuracy: 0.9330 - val_loss: 0.1841\n",
      "Epoch 7/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - accuracy: 0.9356 - loss: 0.1702 - val_accuracy: 0.9330 - val_loss: 0.1841\n",
      "Epoch 7/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step - accuracy: 0.9363 - loss: 0.1660 - val_accuracy: 0.9330 - val_loss: 0.1804\n",
      "Epoch 8/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step - accuracy: 0.9363 - loss: 0.1660 - val_accuracy: 0.9330 - val_loss: 0.1804\n",
      "Epoch 8/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.9417 - loss: 0.1559 - val_accuracy: 0.9425 - val_loss: 0.1701\n",
      "Epoch 9/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.9417 - loss: 0.1559 - val_accuracy: 0.9425 - val_loss: 0.1701\n",
      "Epoch 9/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.9439 - loss: 0.1456 - val_accuracy: 0.9417 - val_loss: 0.1711\n",
      "Epoch 10/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.9439 - loss: 0.1456 - val_accuracy: 0.9417 - val_loss: 0.1711\n",
      "Epoch 10/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - accuracy: 0.9470 - loss: 0.1383 - val_accuracy: 0.9451 - val_loss: 0.1655\n",
      "Epoch 11/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - accuracy: 0.9470 - loss: 0.1383 - val_accuracy: 0.9451 - val_loss: 0.1655\n",
      "Epoch 11/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - accuracy: 0.9506 - loss: 0.1317 - val_accuracy: 0.9421 - val_loss: 0.1651\n",
      "Epoch 12/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - accuracy: 0.9506 - loss: 0.1317 - val_accuracy: 0.9421 - val_loss: 0.1651\n",
      "Epoch 12/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - accuracy: 0.9515 - loss: 0.1293 - val_accuracy: 0.9456 - val_loss: 0.1566\n",
      "Epoch 13/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - accuracy: 0.9515 - loss: 0.1293 - val_accuracy: 0.9456 - val_loss: 0.1566\n",
      "Epoch 13/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - accuracy: 0.9531 - loss: 0.1250 - val_accuracy: 0.9425 - val_loss: 0.1581\n",
      "Epoch 14/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - accuracy: 0.9531 - loss: 0.1250 - val_accuracy: 0.9425 - val_loss: 0.1581\n",
      "Epoch 14/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - accuracy: 0.9572 - loss: 0.1189 - val_accuracy: 0.9451 - val_loss: 0.1559\n",
      "Epoch 15/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - accuracy: 0.9572 - loss: 0.1189 - val_accuracy: 0.9451 - val_loss: 0.1559\n",
      "Epoch 15/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - accuracy: 0.9572 - loss: 0.1140 - val_accuracy: 0.9460 - val_loss: 0.1538\n",
      "Epoch 16/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - accuracy: 0.9572 - loss: 0.1140 - val_accuracy: 0.9460 - val_loss: 0.1538\n",
      "Epoch 16/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - accuracy: 0.9571 - loss: 0.1136 - val_accuracy: 0.9478 - val_loss: 0.1494\n",
      "Epoch 17/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - accuracy: 0.9571 - loss: 0.1136 - val_accuracy: 0.9478 - val_loss: 0.1494\n",
      "Epoch 17/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - accuracy: 0.9598 - loss: 0.1084 - val_accuracy: 0.9491 - val_loss: 0.1555\n",
      "Epoch 18/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - accuracy: 0.9598 - loss: 0.1084 - val_accuracy: 0.9491 - val_loss: 0.1555\n",
      "Epoch 18/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - accuracy: 0.9609 - loss: 0.1068 - val_accuracy: 0.9525 - val_loss: 0.1472\n",
      "Epoch 19/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - accuracy: 0.9609 - loss: 0.1068 - val_accuracy: 0.9525 - val_loss: 0.1472\n",
      "Epoch 19/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - accuracy: 0.9611 - loss: 0.1023 - val_accuracy: 0.9512 - val_loss: 0.1457\n",
      "Epoch 20/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - accuracy: 0.9611 - loss: 0.1023 - val_accuracy: 0.9512 - val_loss: 0.1457\n",
      "Epoch 20/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - accuracy: 0.9637 - loss: 0.0965 - val_accuracy: 0.9521 - val_loss: 0.1446\n",
      "Epoch 21/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - accuracy: 0.9637 - loss: 0.0965 - val_accuracy: 0.9521 - val_loss: 0.1446\n",
      "Epoch 21/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - accuracy: 0.9617 - loss: 0.1001 - val_accuracy: 0.9543 - val_loss: 0.1443\n",
      "Epoch 22/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - accuracy: 0.9617 - loss: 0.1001 - val_accuracy: 0.9543 - val_loss: 0.1443\n",
      "Epoch 22/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488us/step - accuracy: 0.9639 - loss: 0.0949 - val_accuracy: 0.9530 - val_loss: 0.1445\n",
      "Epoch 23/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488us/step - accuracy: 0.9639 - loss: 0.0949 - val_accuracy: 0.9530 - val_loss: 0.1445\n",
      "Epoch 23/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - accuracy: 0.9709 - loss: 0.0865 - val_accuracy: 0.9521 - val_loss: 0.1465\n",
      "Epoch 24/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - accuracy: 0.9709 - loss: 0.0865 - val_accuracy: 0.9521 - val_loss: 0.1465\n",
      "Epoch 24/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - accuracy: 0.9691 - loss: 0.0863 - val_accuracy: 0.9586 - val_loss: 0.1419\n",
      "Epoch 25/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - accuracy: 0.9691 - loss: 0.0863 - val_accuracy: 0.9586 - val_loss: 0.1419\n",
      "Epoch 25/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - accuracy: 0.9683 - loss: 0.0864 - val_accuracy: 0.9552 - val_loss: 0.1455\n",
      "Epoch 26/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - accuracy: 0.9683 - loss: 0.0864 - val_accuracy: 0.9552 - val_loss: 0.1455\n",
      "Epoch 26/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - accuracy: 0.9682 - loss: 0.0857 - val_accuracy: 0.9486 - val_loss: 0.1500\n",
      "Epoch 27/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - accuracy: 0.9682 - loss: 0.0857 - val_accuracy: 0.9486 - val_loss: 0.1500\n",
      "Epoch 27/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 0.9692 - loss: 0.0795 - val_accuracy: 0.9560 - val_loss: 0.1386\n",
      "Epoch 28/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 0.9692 - loss: 0.0795 - val_accuracy: 0.9560 - val_loss: 0.1386\n",
      "Epoch 28/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480us/step - accuracy: 0.9704 - loss: 0.0776 - val_accuracy: 0.9521 - val_loss: 0.1455\n",
      "Epoch 29/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480us/step - accuracy: 0.9704 - loss: 0.0776 - val_accuracy: 0.9521 - val_loss: 0.1455\n",
      "Epoch 29/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481us/step - accuracy: 0.9730 - loss: 0.0748 - val_accuracy: 0.9578 - val_loss: 0.1405\n",
      "Epoch 30/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481us/step - accuracy: 0.9730 - loss: 0.0748 - val_accuracy: 0.9578 - val_loss: 0.1405\n",
      "Epoch 30/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482us/step - accuracy: 0.9730 - loss: 0.0738 - val_accuracy: 0.9582 - val_loss: 0.1406\n",
      "Epoch 31/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482us/step - accuracy: 0.9730 - loss: 0.0738 - val_accuracy: 0.9582 - val_loss: 0.1406\n",
      "Epoch 31/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step - accuracy: 0.9721 - loss: 0.0806 - val_accuracy: 0.9525 - val_loss: 0.1423\n",
      "Epoch 32/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step - accuracy: 0.9721 - loss: 0.0806 - val_accuracy: 0.9525 - val_loss: 0.1423\n",
      "Epoch 32/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - accuracy: 0.9729 - loss: 0.0723 - val_accuracy: 0.9582 - val_loss: 0.1408\n",
      "Epoch 33/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - accuracy: 0.9729 - loss: 0.0723 - val_accuracy: 0.9582 - val_loss: 0.1408\n",
      "Epoch 33/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - accuracy: 0.9746 - loss: 0.0718 - val_accuracy: 0.9621 - val_loss: 0.1408\n",
      "Epoch 34/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - accuracy: 0.9746 - loss: 0.0718 - val_accuracy: 0.9621 - val_loss: 0.1408\n",
      "Epoch 34/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - accuracy: 0.9740 - loss: 0.0711 - val_accuracy: 0.9595 - val_loss: 0.1431\n",
      "Epoch 35/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - accuracy: 0.9740 - loss: 0.0711 - val_accuracy: 0.9595 - val_loss: 0.1431\n",
      "Epoch 35/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - accuracy: 0.9747 - loss: 0.0665 - val_accuracy: 0.9573 - val_loss: 0.1449\n",
      "Epoch 36/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - accuracy: 0.9747 - loss: 0.0665 - val_accuracy: 0.9573 - val_loss: 0.1449\n",
      "Epoch 36/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - accuracy: 0.9735 - loss: 0.0743 - val_accuracy: 0.9569 - val_loss: 0.1416\n",
      "Epoch 37/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - accuracy: 0.9735 - loss: 0.0743 - val_accuracy: 0.9569 - val_loss: 0.1416\n",
      "Epoch 37/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - accuracy: 0.9753 - loss: 0.0670 - val_accuracy: 0.9595 - val_loss: 0.1376\n",
      "Epoch 38/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - accuracy: 0.9753 - loss: 0.0670 - val_accuracy: 0.9595 - val_loss: 0.1376\n",
      "Epoch 38/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - accuracy: 0.9741 - loss: 0.0657 - val_accuracy: 0.9617 - val_loss: 0.1360\n",
      "Epoch 39/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - accuracy: 0.9741 - loss: 0.0657 - val_accuracy: 0.9617 - val_loss: 0.1360\n",
      "Epoch 39/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - accuracy: 0.9723 - loss: 0.0725 - val_accuracy: 0.9586 - val_loss: 0.1430\n",
      "Epoch 40/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - accuracy: 0.9723 - loss: 0.0725 - val_accuracy: 0.9586 - val_loss: 0.1430\n",
      "Epoch 40/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481us/step - accuracy: 0.9782 - loss: 0.0612 - val_accuracy: 0.9582 - val_loss: 0.1433\n",
      "Epoch 41/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481us/step - accuracy: 0.9782 - loss: 0.0612 - val_accuracy: 0.9582 - val_loss: 0.1433\n",
      "Epoch 41/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483us/step - accuracy: 0.9783 - loss: 0.0614 - val_accuracy: 0.9595 - val_loss: 0.1413\n",
      "Epoch 42/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483us/step - accuracy: 0.9783 - loss: 0.0614 - val_accuracy: 0.9595 - val_loss: 0.1413\n",
      "Epoch 42/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482us/step - accuracy: 0.9802 - loss: 0.0588 - val_accuracy: 0.9604 - val_loss: 0.1383\n",
      "Epoch 43/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482us/step - accuracy: 0.9802 - loss: 0.0588 - val_accuracy: 0.9604 - val_loss: 0.1383\n",
      "Epoch 43/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - accuracy: 0.9777 - loss: 0.0598 - val_accuracy: 0.9591 - val_loss: 0.1462\n",
      "Epoch 44/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - accuracy: 0.9777 - loss: 0.0598 - val_accuracy: 0.9591 - val_loss: 0.1462\n",
      "Epoch 44/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.9781 - loss: 0.0572 - val_accuracy: 0.9586 - val_loss: 0.1514\n",
      "Epoch 45/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.9781 - loss: 0.0572 - val_accuracy: 0.9586 - val_loss: 0.1514\n",
      "Epoch 45/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - accuracy: 0.9794 - loss: 0.0566 - val_accuracy: 0.9591 - val_loss: 0.1509\n",
      "Epoch 46/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - accuracy: 0.9794 - loss: 0.0566 - val_accuracy: 0.9591 - val_loss: 0.1509\n",
      "Epoch 46/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - accuracy: 0.9794 - loss: 0.0545 - val_accuracy: 0.9599 - val_loss: 0.1505\n",
      "Epoch 47/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - accuracy: 0.9794 - loss: 0.0545 - val_accuracy: 0.9599 - val_loss: 0.1505\n",
      "Epoch 47/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - accuracy: 0.9812 - loss: 0.0527 - val_accuracy: 0.9586 - val_loss: 0.1481\n",
      "Epoch 48/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - accuracy: 0.9812 - loss: 0.0527 - val_accuracy: 0.9586 - val_loss: 0.1481\n",
      "Epoch 48/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - accuracy: 0.9801 - loss: 0.0539 - val_accuracy: 0.9604 - val_loss: 0.1486\n",
      "Epoch 49/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - accuracy: 0.9801 - loss: 0.0539 - val_accuracy: 0.9604 - val_loss: 0.1486\n",
      "Epoch 49/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.9800 - loss: 0.0612 - val_accuracy: 0.9591 - val_loss: 0.1483\n",
      "Epoch 50/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.9800 - loss: 0.0612 - val_accuracy: 0.9591 - val_loss: 0.1483\n",
      "Epoch 50/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - accuracy: 0.9789 - loss: 0.0559 - val_accuracy: 0.9608 - val_loss: 0.1452\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - accuracy: 0.9789 - loss: 0.0559 - val_accuracy: 0.9608 - val_loss: 0.1452\n"
     ]
    }
   ],
   "source": [
    "# ===== 6) 콜백 & 학습 =====\n",
    "# class StopAtAcc(keras.callbacks.Callback):\n",
    "#     def __init__(self, target=EARLY_STOP_AT_ACC): super().__init__(); self.target=target\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         if logs and logs.get(\"accuracy\",0) >= self.target:\n",
    "#             print(f\"\\nReached {self.target*100:.0f}% accuracy — stopping.\")\n",
    "#             self.model.stop_training = True\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[StopAtAcc()]\n",
    "# )\n",
    "# EarlyStopping 콜백을 주석 처리합니다 — 필요 시 다시 활성화하세요.\n",
    "# es = keras.callbacks.EarlyStopping(\n",
    "#     monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "# )\n",
    "\n",
    "# Prepare sample weights when using soft labels — optional. Set APPLY_SAMPLE_WEIGHT=True to enable.\n",
    "sample_weight = None\n",
    "val_sample_weight = None\n",
    "if LABEL_MODE == 'soft' and APPLY_SAMPLE_WEIGHT:\n",
    "    # down-weight samples whose soft label is near 0.5 (uncertain).\n",
    "    sw = np.abs(y_train - 0.5) * 2.0  # maps distance to 0..1\n",
    "    sw = np.clip(sw, 0.1, 1.0)  # floor to avoid zero weights\n",
    "    sample_weight = sw\n",
    "    vsw = np.abs(y_val - 0.5) * 2.0\n",
    "    val_sample_weight = np.clip(vsw, 0.1, 1.0)\n",
    "    print('Sample weight [train] min/max/mean:', sample_weight.min(), sample_weight.max(), sample_weight.mean())\n",
    "elif LABEL_MODE == 'soft':\n",
    "    print('APPLY_SAMPLE_WEIGHT is False: training will use soft labels directly (0.5 included)')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val, val_sample_weight) if val_sample_weight is not None else (X_val, y_val),\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "    # EarlyStopping disabled — callbacks list left empty intentionally\n",
    "    callbacks=[],\n",
    "    sample_weight=sample_weight,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d48e2252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483us/step\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483us/step\n",
      "Validation results: logloss=0.144978, AUC=0.9898, accuracy=0.9608\n",
      "preds mean/min/max: 0.49977952 1.9314475e-19 1.0\n",
      "soft-label stats: min/max/mean 0.0 1.0 0.5002177\n",
      "Validation results: logloss=0.144978, AUC=0.9898, accuracy=0.9608\n",
      "preds mean/min/max: 0.49977952 1.9314475e-19 1.0\n",
      "soft-label stats: min/max/mean 0.0 1.0 0.5002177\n"
     ]
    }
   ],
   "source": [
    "# ===== EVAL: validation metrics (soft & binary aware) =====\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# predict probabilities\n",
    "preds = model.predict(X_val).flatten()\n",
    "eps = 1e-7\n",
    "preds_clipped = np.clip(preds, eps, 1.0 - eps)\n",
    "# decide which true labels to use\n",
    "if LABEL_MODE == 'soft':\n",
    "    y_true_soft = y_val\n",
    "    y_true_binary = (y_val >= 0.5).astype('float32')\n",
    "elif LABEL_MODE == 'binary':\n",
    "    y_true_binary = y_val\n",
    "    y_true_soft = y_soft_val if 'y_soft_val' in globals() else y_true_binary.astype('float32')\n",
    "else:  # both\n",
    "    y_true_soft = y_soft_val\n",
    "    y_true_binary = y_binary_val\n",
    "# logloss (works with soft targets)\n",
    "logloss = -np.mean(y_true_soft * np.log(preds_clipped) + (1 - y_true_soft) * np.log(1 - preds_clipped))\n",
    "# AUC & Accuracy (binary metrics)\n",
    "auc = roc_auc_score(y_true_binary, preds)\n",
    "acc = np.mean((preds >= 0.5).astype('float32') == y_true_binary)\n",
    "print('Validation results: logloss={:.6f}, AUC={:.4f}, accuracy={:.4f}'.format(logloss, auc, acc))\n",
    "print('preds mean/min/max:', preds.mean(), preds.min(), preds.max())\n",
    "print('soft-label stats: min/max/mean', y_true_soft.min(), y_true_soft.max(), y_true_soft.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "378d8e4c-dce4-4fd9-93de-0349c29e21dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAATSVJREFUeJzt3Xl8U1XC//FvljbdW9rSjaUUBWQRFBAEZVAQFBTFZQRFWcRnxB3RUREVRZ9BHUd91AEXNhcUZBTlJ6hUWVVUdhEYXFgKUigt0L1JmtzfH2kDoQXa0jYlfN6v130lOXfJyelyvzn33HtNhmEYAgAACBBmf1cAAACgNhFuAABAQCHcAACAgEK4AQAAAYVwAwAAAgrhBgAABBTCDQAACCiEGwAAEFAINwAAIKAQboB6ZjKZqjQtW7bslN7nqaeekslkqtG6y5Ytq5U6NDTXXnutQkNDdfjw4eMuM2zYMAUFBWn//v1V3q7JZNJTTz3lfV2d9hs5cqRatGhR5fc62pQpUzRr1qwK5Tt37pTJZKp0HnAmsPq7AsCZZtWqVT6vn3nmGS1dulRLlizxKW/Xrt0pvc/tt9+uK664okbrdu7cWatWrTrlOjQ0o0eP1qeffqoPPvhAd911V4X5ubm5mj9/vq666iolJibW+H3qq/2mTJmi+Ph4jRw50qc8OTlZq1at0llnnVWn7w80VIQboJ5deOGFPq8bN24ss9lcofxYRUVFCgsLq/L7NG3aVE2bNq1RHaOiok5an9PRgAEDlJKSohkzZlQabj788EMVFxdr9OjRp/Q+/m4/m80WkD8/oKo4LAU0QJdccok6dOigFStWqGfPngoLC9Ntt90mSZo7d6769++v5ORkhYaGqm3btnr00UdVWFjos43KDku1aNFCV111lb788kt17txZoaGhOuecczRjxgyf5So7rDJy5EhFRETo999/18CBAxUREaFmzZrpwQcflN1u91l/z549uuGGGxQZGamYmBgNGzZMq1evPumhko0bN8pkMmn69OkV5n3xxRcymUxasGCBJOnAgQP629/+pmbNmslms6lx48a66KKL9PXXXx93+xaLRSNGjNDatWu1adOmCvNnzpyp5ORkDRgwQAcOHNBdd92ldu3aKSIiQgkJCerTp49Wrlx53O2XO95hqVmzZqlNmzay2Wxq27at3n333UrXf/rpp9W9e3fFxsYqKipKnTt31vTp03X0fY5btGihzZs3a/ny5d5DmeWHt453WOrbb79V3759FRkZqbCwMPXs2VMLFy6sUEeTyaSlS5fqzjvvVHx8vOLi4nTddddp7969J/3sQENAuAEaqMzMTN1yyy26+eabtWjRIm9Pw2+//aaBAwdq+vTp+vLLLzV27Fh99NFHGjRoUJW2u3HjRj344IN64IEH9Nlnn6ljx44aPXq0VqxYcdJ1nU6nrr76avXt21efffaZbrvtNr388st6/vnnvcsUFhbq0ksv1dKlS/X888/ro48+UmJiooYMGXLS7Xfq1Ennn3++Zs6cWWHerFmzlJCQoIEDB0qSbr31Vn366ad68skntXjxYk2bNk2XXXaZcnJyTvget912m0wmU4VAt2XLFv30008aMWKELBaLDh48KEmaOHGiFi5cqJkzZ6ply5a65JJLajQWadasWRo1apTatm2rjz/+WI8//rieeeaZCocjJU84ueOOO/TRRx/pk08+0XXXXad7771XzzzzjHeZ+fPnq2XLljr//PO1atUqrVq1SvPnzz/u+y9fvlx9+vRRbm6upk+frg8//FCRkZEaNGiQ5s6dW2H522+/XUFBQfrggw/0wgsvaNmyZbrllluq/bkBvzAA+NWIESOM8PBwn7LevXsbkoxvvvnmhOu63W7D6XQay5cvNyQZGzdu9M6bOHGiceyfeGpqqhESEmLs2rXLW1ZcXGzExsYad9xxh7ds6dKlhiRj6dKlPvWUZHz00Uc+2xw4cKDRpk0b7+t///vfhiTjiy++8FnujjvuMCQZM2fOPOFnevXVVw1JxrZt27xlBw8eNGw2m/Hggw96yyIiIoyxY8eecFvH07t3byM+Pt5wOBzesgcffNCQZPz666+VrlNaWmo4nU6jb9++xrXXXuszT5IxceJE7+tj28/lchkpKSlG586dDbfb7V1u586dRlBQkJGamnrcurpcLsPpdBqTJk0y4uLifNZv37690bt37wrr7Nixo0JbX3jhhUZCQoKRn5/v85k6dOhgNG3a1LvdmTNnGpKMu+66y2ebL7zwgiHJyMzMPG5dgYaCnhuggWrUqJH69OlToXz79u26+eablZSUJIvFoqCgIPXu3VuStHXr1pNu97zzzlPz5s29r0NCQtS6dWvt2rXrpOuaTKYKPUQdO3b0WXf58uWKjIysMJj5pptuOun2Jc/ZSjabzeeQyocffii73a5Ro0Z5y7p166ZZs2bp2Wef1Q8//CCn01ml7UuegcXZ2dneQ1ylpaV6//331atXL7Vq1cq73BtvvKHOnTsrJCREVqtVQUFB+uabb6rUzkfbtm2b9u7dq5tvvtnnUGFqaqp69uxZYfklS5bosssuU3R0tPdn/OSTTyonJ0dZWVnVem/J05v2448/6oYbblBERIS33GKx6NZbb9WePXu0bds2n3Wuvvpqn9cdO3aUpCr9ngD+RrgBGqjk5OQKZQUFBerVq5d+/PFHPfvss1q2bJlWr16tTz75RJJUXFx80u3GxcVVKLPZbFVaNywsTCEhIRXWLSkp8b7Oycmp9Eyjqp59FBsbq6uvvlrvvvuuXC6XJM8hnW7duql9+/be5ebOnasRI0Zo2rRp6tGjh2JjYzV8+HDt27fvpO9xww03KDo62nv4a9GiRdq/f7/PQOKXXnpJd955p7p3766PP/5YP/zwg1avXq0rrriiSm11tPJDZUlJSRXmHVv2008/qX///pKkt99+W999951Wr16tCRMmSKraz/hYhw4dkmEYlf5OpaSk+NSx3LG/JzabrcbvD9Q3zpYCGqjKrlGzZMkS7d27V8uWLfP21kg64XVb6ltcXJx++umnCuVVCR3lRo0apXnz5ik9PV3NmzfX6tWrNXXqVJ9l4uPj9corr+iVV15RRkaGFixYoEcffVRZWVn68ssvT7j90NBQ3XTTTXr77beVmZmpGTNmKDIyUn/961+9y7z//vu65JJLKrxvfn5+lT9HufKgUFkbHFs2Z84cBQUF6fPPP/cJkp9++mm137dco0aNZDablZmZWWFe+SDh+Pj4Gm8faGjouQFOI+WBp/xbdLk333zTH9WpVO/evZWfn68vvvjCp3zOnDlV3kb//v3VpEkTzZw5UzNnzlRISMgJD2s1b95c99xzj/r166d169ZV6T1Gjx4tl8ulf/7zn1q0aJGGDh3qc6q9yWSq0M4///xzhesUVUWbNm2UnJysDz/80OeMp127dun777/3WdZkMslqtcpisXjLiouL9d5771XYblV73MLDw9W9e3d98sknPsu73W69//77atq0qVq3bl3tzwU0VIQb4DTSs2dPNWrUSGPGjNH8+fP1+eef66abbtLGjRv9XTWvESNG6Oyzz9Ytt9yiqVOnKj09XePGjdNXX30lSTKbT/5vx2KxaPjw4frss8/0zjvv6LrrrlN0dLR3fm5urjp37qwXX3xRn3/+uZYvX64XX3xRX375pfr161elenbt2lUdO3bUK6+8IqfTWeHaNldddZUWL16siRMnasmSJZo6daouv/xypaWlVaM1PMxms5555hmtXbtW1157rRYuXKjZs2frsssuq3BY6sorr1RBQYFuvvlmpaena86cOerVq1eFoCVJ5557rjZu3Ki5c+dq9erVlZ7eXm7y5MnKycnRpZdeqv/85z9asGCBBg4cqF9++UUvvvhija9mDTREhBvgNBIXF6eFCxcqLCxMt9xyi2677TZFRERUeiqvv4SHh2vJkiW65JJL9PDDD+v6669XRkaGpkyZIkmKiYmp0nZGjRolu92uAwcO+AwkljyDoLt376733ntPw4YN04ABAzRt2jQ98sgjevvtt6tc19GjR8swDLVr107du3f3mTdhwgQ9+OCDmj59uq688kpNmzZNb7zxhi6++OIqb//Y95o2bZq2bNmi6667TpMmTdJjjz1WYdB4nz59NGPGDG3atEmDBg3ShAkTdMMNN+jRRx+tsM2nn35avXv31v/8z/+oW7duJ7wcQO/evbVkyRKFh4dr5MiRGjp0qHJzc7VgwYIqnaYPnE5MxtF9pABQR/7xj3/o8ccfV0ZGRo2vnAwAVcGAYgC17vXXX5cknXPOOXI6nVqyZIleffVV3XLLLQQbAHWOcAOg1oWFhenll1/Wzp07Zbfb1bx5cz3yyCN6/PHH/V01AGcADksBAICA4tcBxStWrNCgQYOUkpIik8lUpes4LF++XF26dFFISIhatmypN954o+4rCgAATht+DTeFhYXq1KmT9/j8yezYsUMDBw5Ur169tH79ej322GO677779PHHH9dxTQEAwOmiwRyWMplMmj9/vgYPHnzcZR555BEtWLDA574uY8aM0caNG2t0YS0AABB4TqsBxatWrfLec6Xc5ZdfrunTp8vpdCooKKjCOna7XXa73fva7Xbr4MGDiouL46JVAACcJgzDUH5+vlJSUk56MdDTKtzs27evws33EhMTVVpaquzs7EpvCjd58mQ9/fTT9VVFAABQh3bv3n3SS0qcVuFGqngzwfKjasfrhRk/frzGjRvnfZ2bm6vmzZtr9+7dioqKqruKAgCAWpOXl6dmzZopMjLypMueVuEmKSmpwh10s7KyZLVavXfdPZbNZqv0nixRUVGEGwAATjNVGVJyWt1bqkePHkpPT/cpW7x4sbp27VrpeBsAAHDm8Wu4KSgo0IYNG7RhwwZJnlO9N2zYoIyMDEmeQ0rDhw/3Lj9mzBjt2rVL48aN09atWzVjxgxNnz5dDz30kD+qDwAAGiC/HpZas2aNLr30Uu/r8rExI0aM0KxZs5SZmekNOpKUlpamRYsW6YEHHtC///1vpaSk6NVXX9X1119f73UHAAANU4O5zk19ycvLU3R0tHJzcxlzAwDwYS916XCRU4eKHDpU6NThIocOlb0+XOSQ02WocaRNCZE2JUaFKCHKpsTIEMWEBdX48iKGYche6lZ+SakK7aUqKJsK7aUqdZ98F13idCm32KnDRWVTsUO5RU4dLvbUP7e4VMWOUoUGWxUWbFFYsEWhZY9hR5WFBFlkNplkkmQyeca2mEySSeWPktlkks1qVmiwReE2z7qhQWXbsZVtM8jzPD6i4njXU1Gd/fdpNaAYAICq+PNwsdbsPKifdhzUuozDyit2yjAMGZIMQzJklD16XkuGihwuFTlcNXq/YItZjSNtSoyyKS7CJsOQSt1uudyGSl2GSt1ulboNudyGnC5DpS63ihwub5BxVSHEnKrCGn62mogND9a6J/rV2/sdi3ADAKc5t9tQvr207Nu6o+zbu1O5RQ7lFjuVby+V1WxSsMWiYKvZM1lMRz23yGoxyeU2ZC91ye50y17q9j53uMpeO10KspgVZrMqPNiiMJtVETbPt/bwYKvCbZ5v827D8PYi5B5Vn6Nf20vdCgu2KLzsG3/5Y0Sw1bv9cJtV0aFBigkLUkxosKLDghRps8psNlX4/L9lFWj1zoNavfOg1uw8pD8PF9e4Pc0mKSYsWDFhQWoUFqxGYUGKKXsMsph1IN+u/fl2ZeWVaH9eiQ4VOeVwufXn4eJTel9JirB52jHCZlWEzSqr5eRDY4MtZjUKD1J0aPBR7eV5jA71fI6QIIuKHS4VO0u9Ia647LHI4SkrcbrkPjr4Gb4B0G0YMgxDJU63ipwuFTt8t1XoKPVuMyzYckrtcKoINwDqldPl1u6DRYqwWRUTFqxg6+lz0qbLbSinwK79eXbtyyvRvrwS7c8te8wr0b7cEh0osFfpW3iQxawQq1khQRbZgiwKCTIrxFr2GGSRzWqW2WQ6EjJK3WWhw/PcUeoJHIWOUuUVO1UPX/wbBLNJZTtwz448JMisrZn5yi12+ixnMZvUISVKXVvE6oIWjZQcHepziEWS7yEXkxRitahRWLAiQyoGqBOxl7o8gSfPE3hyCh2ymE2ymE0KsphkMZsV5H1tlsVsktVsKguHZVOIVWFBlmq9b0Pm7xEvhBsAdW7v4WIt//WAlm3L0ne/56jAXuqdFx5s8XwrDvd8S4456ptyaNlO3hZkls3q6XWweSeLbEFmWY4ZFyD57rTMJpOcLrdKnC4VO11l314931KLyp+XfdssLPsWW2h3qdBe6nnucKnI7nmsr8MHNRUaZCn7tn6ktyMmLEjhNqtcbkMOlycUOUrdcpY/dx15bbX4tq1PW5f18jhdhoocnkMpRXbPt3VPW5U/d8kked4/LFgxoZ76RB9Vn5iwINmsFhU7j7T10e1c3vb5JaXKLXZ6x5MUl/UseMbAOCt89vObx+iCFrHqlhar85rFKNxWP7s4m9Wipo3C1LRRWL283+nA37c3ItwAqHX2UpfW7jykZWWB5tf9BT7zQ4Msspd6dlSFDpcKHafenV9fzCYpPsKmpOgQJUaFKCkqxOd5QpRNQSc5lGAYhkrdhkqcLpU43WWPLpWUep7by8rdhuHtxSkPeOVhozzohQZbFBMapKhQz6GHQFbidCmv+MghrsNFDhXYS9WycYTap0SdtN1x5iDcAGcYwzB0qMjpOYxSdlglK98uq8Wk+HCbYsODFRsR7HkeEazwYEultz3JKy7VgQK7cgrsyi5wKKfQ87hlb56+/yPbZ2Cm2SSd1yxGl7RJ0CVtGqtDSrQkKb+k1HNWSvlUWH5WimfsSInzyFgPh8v3sEz54Rq3WycdKGo2mbxng5SfKRIa5HkdGnTkdfnZH+WPETarZzxJ2biSyBCr4sKDqzQOArUvpOxnlhAV4u+qoIEj3AANUKnLrcPFThXZXSoqGwBYXNZdX+x0eQfxlThdcrsNucsG+x2Zygf/eca4eMYDlI8NsctR6q5yXYKtZsWFBysuIliGIWUX2HWw0HNK7InER9jUu3VjXdKmsXq1ildMWHCFZaLDPIcrWii82m2ESpQ6pEM7pOxfpQPbpOzfPM/z9krh8VJkshSVLEWmHHmMTJKiUqSwOKk+DyW43VLJYanwwJGpoPx5llSYLdnzpIgkKaaZFNNcii5/bCoFhdZRvVxSaYnkLJFKiys+Gm4pOMIz2SIlW9lzcz32mhmGVGo/Ui+TyVOXoLDq/QzdLqkkVyo+5Jns+Z5t2Mo+W/lntBznDgDl65ccPrKN4rLnJrN0weja+LQ1QrgBqsAwDOUUOlR6kh16ufJrQhy5TkTZ87J5+SWlyswtUWZusfblligz1zMYNTOvRPtyi3Ug317nA0TjwoOVEBWipCjP9TocLrcOFjp0sNChnLKemBKnZzxGZlkdjxUZYlXjCJviIoIVF25TfGSwmsSEqVereLVLjqrZ4MjDGdL25dKfa6WwWKlRmhTb0jNFJtXvDri6CrKkvRskZ6HnH7/LKblLJbfzmNelnp10s+6eHXdN5P4p7f5RytxYFmK2SQd3SMZxTvct2Cft/+X427MES+EJnhAU3liKKH+e4HkdHu8pCwqTzFbPDs9s9Z3KyxwFUl6mlF825e096nnZY8F+TzvUVHiCp+2im0nB4Z5wYQ46qh5Hv7Z6woC9wLMDd+R7Hu0Fnrray147Cj0/q5oICj8q7IR72rOytjl6Mlxlvw8n+F05OsR4H0vk6Zs8hsl8JJAER/iGFEvQkeBRHkLsuVX7bNaQo7Yb7mmn4kOeYFNZPSRPkCbcAA2DYRjKzC3Rb1kF+m1/vn7bX6Bfs/L1+/4C5dtP4R9xDYWWHUYJs3kujHXshbdCgsyymM0ylw2cNZddeMtiNh0pM5sUH+G5/kZSVIj3wmM268m/aRY5SpVT4Ak8Bws84SY+MtQTZiKCq7SNkyrMkXaukLYv84SaQzuOv2xQWFnYKQ88aVLyeVJyp5p9c3a7pcz10q9fSbl7PNuLby3Ft/Fs31qxt+nIui4pa6snYOz+yfN4orofT1QTqVk3T9Bp1k1K6ljxm7LLKe3bdOR9dv8k5e2pfHvBEVJ8K89niG/l+TzRTaWig1L+3rJwccxjUbbkcni2ebzt1pWQ6IoBKryxZwqO8ASh3N2e0Ht4t+e5o6CsdyfLE4LriiVYsoZKQSGeHbw1xBMgHIVHAlJ5QHMWeqaCE2+y1pnMZcdfDU+vkj3PM1VHcKQU2sgThpxFR0Kgy+6ZX1oWqIqyj7N+hGf9kBgpNMbzPDLpFD7UqeMKxQho5Vf+zCtxKq+4tOzRqfySUp+y7Hy7fssq0O9ZBT5n8hzLWoWeCEO+Y0AqE2w1KznaMwA1OTpESdGhZY8h3se4cJsslb2fq9SzAzq4Xcrf7+met0Uembzd5ZEVd/hu95F/VM5i38eig76HBAoPeHoiyp8XZXv+kR/bJW+L9PxzPPpbq3eHcJxHR5G0c6W0Y7lnp300k0Vq0kVK7eH5J3toh+ezHs7w/POuTEiM1OJiqeUlninu7OP38NjzPUHq1y+lXxd7Pm9lTBapUQup8VEhISxeytzgCRh71lSyEzF5lg+LO05PgtVTZjJJB/4rZf5csafFGio16ewJOiazJ8j8udaz0zm2fkkdpCZdpcbneOrYuI3nG3N1e7dKHZ6enYKjDhFV+nuQ5ek58PYwnCDw26I8O7jIZM9hr8hk38NikUmeAHOiAFkZw/D0GhzO8ASd3D2e3193qedvw33M5HJ66urtfYjw1M2nZ6Oy392Qkwfm8sNDjoKyUFHg2wPkrUOpb5uVv/b+Tpygd8dq8/yNl4erY/+eyoOws+hIb5Q976geqQJPEHM5PaHDG0LKnofGHP+wU6njyHaOfiwPM+Xbqu7PsIaqs/8m3OC0YS91KSOnSDtzirQzu1A7cgp1qNDhPb3Xe6qv06Vix5FTf6t76q7FbFJafLhaJUSoVWKkWiVEqHVipFrEh9W4p+Loi2G5DUNWs+n4p0q6XZ5/IoUHPIcZDm73nQ5nVL3r3Brq+Yftdnp2SuXfxBqahPZSy95SWm8ptacUUsnfZqnDszPztsUOKec3z87/2JARmXJkey17e3olfv3KE2h2fut5XS44Ujq7j6cO3vEqv3p2CCcTHCE17Xqk16VJV8/OoqochdLe9Z6wlPGjtOcnz467MiHRR96nWXcppbNnh+xPhuH5fT16x+12eXbItkj/1g0Bh3BzAoSbhi2/xKk/Dxdrz8Fi7cwp9EzZRdqRXai9ucXH7Qk5GbNJigwJUlSoVZG2sseQIEWVlcWEBuushHC1SohUWnz4qV1YrtRxzAC7SqaS3KOO9x/9ja+g4jf0ylhsnl6FqOSysQT5R40bKPDdeR+3Uay+3wBDY44Za9G44uECs/Wob3HHfKMrf+4orNgrdOyjTJ5Q0PISKe0vnm3XlKvUExB2LPMc1tr948k/f6M0qc0AqfXlUvOeFb95GoaUv88zjqV8UO6BbZ7AmdhBat7dEzAS2tXuQFK3W8r5vaxn6CdPT1XTblLzC6W4VpKZs7Rw5iLcnADhxn8cpZ4Bqwfy7frzcJH2HPJc22TPoWIdzjkgc+5ONXb8qVTTfjUzZalYNu0x4rXHaKw/jXj9acTLERyjFvERahEfrrS4cCVE2byn8oabSxVT8qeiSnYrojBDYQUZsuXtlKW0WOZGzWSKblY2ALG5ZzBnTNlAxGOVOsoGP+7zHZtQkOW74z7RGRW1ISj8yLgS7xiT8oG1KSfe0ZUPnnSUdZGbgyrpzg7QIXfOYinjB8+hpx3LPQN8TWapeQ9PmGl9hecQTkMemAygAsLNCRBu6kZJXrayvntXxq5VKnaZVOAKUr7LqrxSqw47LTrksCjXaVGJglUqi5qYspVq2q8Wpv1KNe1TrKlqo/CMoHCZys+QiGnu6QovP3STt1fHHbl/PKGxnu2ExXnGE5QPrjxlpiMD6yqbbFFHjVk5zvF/a+3eUfeMVZIryVT5oS4Apw3uCo46ZRiGdh8s1vqMHOVtTleLjPnqbv9ezU0nOZvoOGPWypWGJcgU21KW+LOkmFTP4ZnyQYOHd0sF+2RyFnoGYh74b+UbsUVV7OUIjvAMOvQ54yKj7PoOBz3TsSzBZYMhj7oOSGSSZ1vlg/uO9xgSJdmiOYTQUIRE+7sGAOoZ4QbKLXbqQL7d514zzlK37Mfch2bv4WJt2H1Y+3b9qn6Or3W9ZYWamsp6OUzSNrXQppg+CgkNV0xQqSKtpYqwlCrc7FSYyakQORRk2GVyOT1hwXvIpaXUKE3Wkw2OdJZIeX+WBZSy0GOy+AaZsNiqH24oyT1yamlhtmfcR/mZHfV9QTMAQK0h3Jxh3G5Dv2UVaF3GIa3bdUg/78rS4ex9CjGdeACmSYY6mnZohGWZLjJvltnqOfxTbInUvtRBCu02Uq3bdFObugwEQSFS3FmeqTaEREtJ0Z7TaQEAAYNwE6hcTunQThX+uVV/7tmhnH17VHgoU+6CA4p2H1Y35WqgKVfRpiKpBrdpcbXoLUuX4Qo950ql1dVl0AEAqAHCzemuJE+lWdt0KGOzCv/cIiP7N4Xn/aFY+x5Z5VK4pNbHrnPMUBDDZJGpsrOGjhUWJ3W8UTpvmCyNUmvpAwAAULsIN6cZw+3Wn1tXKe+H95SU+Y1iS7NkldS4bDpakWHTdiNZh4ISZIlMVERskuKTmikxqaksUYneS5ybQmIY/AoACBiEm9NAbpFT6zZtkn39HLXZv1Bphu+9X7KMGO0wUpQVkqqS6LNkatxaEU3aKrnZ2WqZEKkOISc5TQkAgABCuGmA3G5D63cf1g//3SXXLwvUJfdL9TZtkdnkGcRbYgRpbdhFOnz2dYppc7FSmyTrgujQmt2BGQCAAEO4aUCcLrcWrv5V65Z9qk6FKzXKvFphJrt3jMzOiPNV3O6vSr34Jl0UFevfygIA0EARbvzN7Zb9z43avOITmX7/Wle6t2mwySWV3a4mLyxV6jRUUd2GqQWDeAEAOCnCjT8UZkt/LJVz22I5f/1aYc6D6lw+zyTlhjZTaNvLFXz+EEU1vYCLyQEAUA2Em/qU9V9p8eMyfv9aJhkKkueOBIWGTessHRXcpp/Ou/R6RSec7e+aAgBw2iLc1Ieig9Ly56Wf3pYMl0ySNrtTtcLdUb9FdtfFfa7UoM4tFGThdGwAAE4V4aYuuUqltTOlpf8rFR+SJKW7L9D/OocqMuUc3X3pWbqjXRJnOQEAUIsIN3Vl+zLpy/FS1hbP64R2Wpr2gP5nebhaJ0ZowT0XycRYGgAAah3hprYd3C4tfkL67+ee16GNpEsnSF1G6e0ZayTl6JrzmhBsAACoI4Sb2mLPl1a8KP0wRXI5JJNF6vY/Uu9HpLBY7cst0artOZKkqzul+LmyAAAELsJNbTmwTfruFc/zs/pIl0+WEs7xzv78570yDKlraiM1iw3zTx0BADgDEG5qS9OuUq+HpKYXSK0vr3Btmk83/ClJuub8Jv6oHQAAZwzCTW3q+0Slxb9nFeiXP/NkNZt05bnJ9VwpAADOLFxYpR4sKOu16dUqXrHhwX6uDQAAgY1wU8cMw9BnG/dKkgZzSAoAgDpHuKljG3Yf1q6cIoUGWXRZ20R/VwcAgIBHuKljn23w9Nr0b5+ocBtDnAAAqGuEmzpU6nLr85894eaa87i2DQAA9YFwU4e+/yNH2QUONQoLUq9Wjf1dHQAAzgiEmzpUfm2bKzsmc8dvAADqCXvcOlLidOmrX/ZJkgafx1lSAADUF8JNHfl6634VOlxqEhOqzs0b+bs6AACcMQg3daT8LKlrzkuR2cwdwAEAqC+EmzpwuMihZduyJEnXcEgKAIB6RbipA1/8sk9Ol6FzkiLVJinS39UBAOCMQripA5+uL7sDOL02AADUO8JNLdt7uFg/7TwoSbqaC/cBAFDvCDe17P9t3CvDkLq1iFWTmFB/VwcAgDMO4aaWec+SOp9eGwAA/IFwU4t+25+vLZl5sppNGtgh2d/VAQDgjES4qUXlvTaXtGmsRuHBfq4NAABnJsJNLTEMQ59t9JwldTVnSQEA4DeEm1qyLuOwdh8sVliwRf3aJvq7OgAAnLGs/q5AoOjUNFrv3tZNew4VKzTY4u/qAABwxiLc1BKrxay/tG7s72oAAHDG47AUAAAIKIQbAAAQUAg3AAAgoBBuAABAQCHcAACAgEK4AQAAAYVwAwAAAorfw82UKVOUlpamkJAQdenSRStXrjzh8rNnz1anTp0UFham5ORkjRo1Sjk5OfVUWwAA0ND5NdzMnTtXY8eO1YQJE7R+/Xr16tVLAwYMUEZGRqXLf/vttxo+fLhGjx6tzZs3a968eVq9erVuv/32eq45AABoqPwabl566SWNHj1at99+u9q2batXXnlFzZo109SpUytd/ocfflCLFi103333KS0tTRdffLHuuOMOrVmzpp5rDgAAGiq/hRuHw6G1a9eqf//+PuX9+/fX999/X+k6PXv21J49e7Ro0SIZhqH9+/frP//5j6688srjvo/dbldeXp7PBAAAApffwk12drZcLpcSE33voJ2YmKh9+/ZVuk7Pnj01e/ZsDRkyRMHBwUpKSlJMTIxee+21477P5MmTFR0d7Z2aNWtWq58DAAA0LH4fUGwymXxeG4ZRoazcli1bdN999+nJJ5/U2rVr9eWXX2rHjh0aM2bMcbc/fvx45ebmeqfdu3fXav0BAEDD4re7gsfHx8tisVTopcnKyqrQm1Nu8uTJuuiii/T3v/9dktSxY0eFh4erV69eevbZZ5WcnFxhHZvNJpvNVvsfAAAANEh+67kJDg5Wly5dlJ6e7lOenp6unj17VrpOUVGRzGbfKlssFkmeHh8AAAC/HpYaN26cpk2bphkzZmjr1q164IEHlJGR4T3MNH78eA0fPty7/KBBg/TJJ59o6tSp2r59u7777jvdd9996tatm1JSUvz1MQAAQAPit8NSkjRkyBDl5ORo0qRJyszMVIcOHbRo0SKlpqZKkjIzM32ueTNy5Ejl5+fr9ddf14MPPqiYmBj16dNHzz//vL8+AgAAaGBMxhl2PCcvL0/R0dHKzc1VVFSUv6sDAACqoDr7b7+fLQUAAFCbCDcAACCgEG4AAEBAIdwAAICAQrgBAAABhXADAAACCuEGAAAEFMINAAAIKIQbAAAQUAg3AAAgoBBuAABAQCHcAACAgEK4AQAAAYVwAwAAAgrhBgAABBTCDQAACCiEGwAAEFAINwAAIKAQbgAAQEAh3AAAgIBCuAEAAAGFcAMAAAIK4QYAAAQUwg0AAAgohBsAABBQCDcAACCgEG4AAEBAIdwAAICAQrgBAAABhXADAAACCuEGAAAEFMINAAAIKIQbAAAQUAg3AAAgoBBuAABAQCHcAACAgEK4AQAAAYVwAwAAAgrhBgAABBTCDQAACCiEGwAAEFAINwAAIKAQbgAAQEAh3AAAgIBCuAEAAAGFcAMAAAIK4QYAAAQUwg0AAAgohBsAABBQCDcAACCgEG4AAEBAIdwAAICAQrgBAAABhXADAAACCuEGAAAEFMINAAAIKIQbAAAQUAg3AAAgoBBuAABAQCHcAACAgEK4AQAAAYVwAwAAAgrhBgAABBS/h5spU6YoLS1NISEh6tKli1auXHnC5e12uyZMmKDU1FTZbDadddZZmjFjRj3VFgAANHRWf7753LlzNXbsWE2ZMkUXXXSR3nzzTQ0YMEBbtmxR8+bNK13nxhtv1P79+zV9+nSdffbZysrKUmlpaT3XHAAANFQmwzAMf7159+7d1blzZ02dOtVb1rZtWw0ePFiTJ0+usPyXX36poUOHavv27YqNja3Re+bl5Sk6Olq5ubmKioqqcd0BAED9qc7+22+HpRwOh9auXav+/fv7lPfv31/ff/99pessWLBAXbt21QsvvKAmTZqodevWeuihh1RcXHzc97Hb7crLy/OZAABA4PLbYans7Gy5XC4lJib6lCcmJmrfvn2VrrN9+3Z9++23CgkJ0fz585Wdna277rpLBw8ePO64m8mTJ+vpp5+u9foDAICGye8Dik0mk89rwzAqlJVzu90ymUyaPXu2unXrpoEDB+qll17SrFmzjtt7M378eOXm5nqn3bt31/pnAAAADYffem7i4+NlsVgq9NJkZWVV6M0pl5ycrCZNmig6Otpb1rZtWxmGoT179qhVq1YV1rHZbLLZbLVbeQAA0GD5recmODhYXbp0UXp6uk95enq6evbsWek6F110kfbu3auCggJv2a+//iqz2aymTZvWaX0BAMDpwa+HpcaNG6dp06ZpxowZ2rp1qx544AFlZGRozJgxkjyHlIYPH+5d/uabb1ZcXJxGjRqlLVu2aMWKFfr73/+u2267TaGhof76GAAAoAHx63VuhgwZopycHE2aNEmZmZnq0KGDFi1apNTUVElSZmamMjIyvMtHREQoPT1d9957r7p27aq4uDjdeOONevbZZ/31EQAAQAPj1+vc+APXuQEA4PRzWlznBgAAoC5UO9y0aNFCkyZN8jlcBAAA0FBUO9w8+OCD+uyzz9SyZUv169dPc+bMkd1ur4u6AQAAVFu1w829996rtWvXau3atWrXrp3uu+8+JScn65577tG6devqoo4AAABVdsoDip1Op6ZMmaJHHnlETqdTHTp00P33369Ro0Yd90rD/sSAYgAATj/V2X/X+FRwp9Op+fPna+bMmUpPT9eFF16o0aNHa+/evZowYYK+/vprffDBBzXdPAAAQI1UO9ysW7dOM2fO1IcffiiLxaJbb71VL7/8ss455xzvMv3799df/vKXWq0oAABAVVQ73FxwwQXq16+fpk6dqsGDBysoKKjCMu3atdPQoUNrpYIAAADVUe1ws337du8VhI8nPDxcM2fOrHGlAAAAaqraZ0tlZWXpxx9/rFD+448/as2aNbVSKQAAgJqqdri5++67tXv37grlf/75p+6+++5aqRQAAEBNVTvcbNmyRZ07d65Qfv7552vLli21UikAAICaqna4sdls2r9/f4XyzMxMWa1+vck4AABA9cNNv379NH78eOXm5nrLDh8+rMcee0z9+vWr1coBAABUV7W7Wv71r3/pL3/5i1JTU3X++edLkjZs2KDExES99957tV5BAACA6qh2uGnSpIl+/vlnzZ49Wxs3blRoaKhGjRqlm266qdJr3gAAANSnGg2SCQ8P19/+9rfargsAAMApq/EI4C1btigjI0MOh8On/Oqrrz7lSgEAANRUja5QfO2112rTpk0ymUwqv6l4+R3AXS5X7dYQAACgGqp9ttT999+vtLQ07d+/X2FhYdq8ebNWrFihrl27atmyZXVQRQAAgKqrds/NqlWrtGTJEjVu3Fhms1lms1kXX3yxJk+erPvuu0/r16+vi3oCAABUSbV7blwulyIiIiRJ8fHx2rt3ryQpNTVV27Ztq93aAQAAVFO1e246dOign3/+WS1btlT37t31wgsvKDg4WG+99ZZatmxZF3UEAACosmqHm8cff1yFhYWSpGeffVZXXXWVevXqpbi4OM2dO7fWKwgAAFAdJqP8dKdTcPDgQTVq1Mh7xlRDlpeXp+joaOXm5ioqKsrf1QEAAFVQnf13tcbclJaWymq16pdffvEpj42NPS2CDQAACHzVCjdWq1WpqalcywYAADRY1T5b6vHHH9f48eN18ODBuqgPAADAKan2gOJXX31Vv//+u1JSUpSamqrw8HCf+evWrau1ygEAAFRXtcPN4MGD66AaAAAAtaNWzpY6nXC2FAAAp586O1sKAACgoav2YSmz2XzC0745kwoAAPhTtcPN/PnzfV47nU6tX79e77zzjp5++ulaqxgAAEBN1NqYmw8++EBz587VZ599VhubqzOMuQEA4PTjlzE33bt319dff11bmwMAAKiRWgk3xcXFeu2119S0adPa2BwAAECNVXvMzbE3yDQMQ/n5+QoLC9P7779fq5UDAACormqHm5dfftkn3JjNZjVu3Fjdu3dXo0aNarVyAAAA1VXtcDNy5Mg6qAYAAEDtqPaYm5kzZ2revHkVyufNm6d33nmnVioFAABQU9UON88995zi4+MrlCckJOgf//hHrVQKAACgpqodbnbt2qW0tLQK5ampqcrIyKiVSgEAANRUtcNNQkKCfv755wrlGzduVFxcXK1UCgAAoKaqHW6GDh2q++67T0uXLpXL5ZLL5dKSJUt0//33a+jQoXVRRwAAgCqr9tlSzz77rHbt2qW+ffvKavWs7na7NXz4cMbcAAAAv6vxvaV+++03bdiwQaGhoTr33HOVmppa23WrE9xbCgCA00919t/V7rkp16pVK7Vq1aqmqwMAANSJao+5ueGGG/Tcc89VKP/nP/+pv/71r7VSKQAAgJqqdrhZvny5rrzyygrlV1xxhVasWFErlQIAAKipaoebgoICBQcHVygPCgpSXl5erVQKAACgpqodbjp06KC5c+dWKJ8zZ47atWtXK5UCAACoqWoPKH7iiSd0/fXX648//lCfPn0kSd98840++OAD/ec//6n1CgIAAFRHtcPN1VdfrU8//VT/+Mc/9J///EehoaHq1KmTlixZwqnVAADA72p8nZtyhw8f1uzZszV9+nRt3LhRLpertupWJ7jODQAAp5/q7L+rPeam3JIlS3TLLbcoJSVFr7/+ugYOHKg1a9bUdHMAAAC1olqHpfbs2aNZs2ZpxowZKiws1I033iin06mPP/6YwcQAAKBBqHLPzcCBA9WuXTtt2bJFr732mvbu3avXXnutLusGAABQbVXuuVm8eLHuu+8+3Xnnndx2AQAANFhV7rlZuXKl8vPz1bVrV3Xv3l2vv/66Dhw4UJd1AwAAqLYqh5sePXro7bffVmZmpu644w7NmTNHTZo0kdvtVnp6uvLz8+uyngAAAFVySqeCb9u2TdOnT9d7772nw4cPq1+/flqwYEFt1q/WcSo4AACnn3o5FVyS2rRpoxdeeEF79uzRhx9+eCqbAgAAqBWnFG7KWSwWDR48uEa9NlOmTFFaWppCQkLUpUsXrVy5skrrfffdd7JarTrvvPOq/Z4AACBw1Uq4qam5c+dq7NixmjBhgtavX69evXppwIABysjIOOF6ubm5Gj58uPr27VtPNQUAAKeLU779wqno3r27OnfurKlTp3rL2rZtq8GDB2vy5MnHXW/o0KFq1aqVLBaLPv30U23YsKHK78mYGwAATj/1NubmVDgcDq1du1b9+/f3Ke/fv7++//774643c+ZM/fHHH5o4cWKV3sdutysvL89nAgAAgctv4SY7O1sul0uJiYk+5YmJidq3b1+l6/z222969NFHNXv2bFmtVbv+4OTJkxUdHe2dmjVrdsp1BwAADZdfx9xIkslk8nltGEaFMklyuVy6+eab9fTTT6t169ZV3v748eOVm5vrnXbv3n3KdQYAAA1XtW6cWZvi4+NlsVgq9NJkZWVV6M2RpPz8fK1Zs0br16/XPffcI0lyu90yDENWq1WLFy9Wnz59Kqxns9lks9nq5kMAAIAGx289N8HBwerSpYvS09N9ytPT09WzZ88Ky0dFRWnTpk3asGGDdxozZozatGmjDRs2qHv37vVVdQAA0ID5redGksaNG6dbb71VXbt2VY8ePfTWW28pIyNDY8aMkeQ5pPTnn3/q3XffldlsVocOHXzWT0hIUEhISIVyAABw5vJruBkyZIhycnI0adIkZWZmqkOHDlq0aJFSU1MlSZmZmSe95g0AAMDR/HqdG3/gOjcAAJx+Tovr3AAAANQFwg0AAAgohBsAABBQCDcAACCgEG4AAEBAIdwAAICAQrgBAAABhXADAAACCuEGAAAEFMINAAAIKIQbAAAQUAg3AAAgoBBuAABAQCHcAACAgEK4AQAAAYVwAwAAAgrhBgAABBTCDQAACCiEGwAAEFAINwAAIKAQbgAAQEAh3AAAgIBCuAEAAAGFcAMAAAIK4QYAAAQUwg0AAAgohBsAABBQCDcAACCgEG4AAEBAIdwAAICAQrgBAAABhXADAAACCuEGAAAEFMINAAAIKIQbAAAQUAg3AAAgoBBuAABAQCHcAACAgEK4AQAAAYVwAwAAAgrhBgAABBTCDQAACCiEGwAAEFAINwAAIKAQbgAAQEAh3AAAgIBCuAEAAAGFcAMAAAIK4QYAAAQUwg0AAAgohBsAABBQCDcAACCgEG4AAEBAIdwAAICAQrgBAAABhXADAAACCuEGAAAEFMINAAAIKIQbAAAQUAg3AAAgoBBuAABAQCHcAACAgEK4AQAAAcXv4WbKlClKS0tTSEiIunTpopUrVx532U8++UT9+vVT48aNFRUVpR49euirr76qx9oCAICGzq/hZu7cuRo7dqwmTJig9evXq1evXhowYIAyMjIqXX7FihXq16+fFi1apLVr1+rSSy/VoEGDtH79+nquOQAAaKhMhmEY/nrz7t27q3Pnzpo6daq3rG3btho8eLAmT55cpW20b99eQ4YM0ZNPPlml5fPy8hQdHa3c3FxFRUXVqN4AAKB+VWf/7beeG4fDobVr16p///4+5f3799f3339fpW243W7l5+crNjb2uMvY7Xbl5eX5TAAAIHD5LdxkZ2fL5XIpMTHRpzwxMVH79u2r0jb+9a9/qbCwUDfeeONxl5k8ebKio6O9U7NmzU6p3gAAoGHz+4Bik8nk89owjApllfnwww/11FNPae7cuUpISDjucuPHj1dubq532r179ynXGQAANFxWf71xfHy8LBZLhV6arKysCr05x5o7d65Gjx6tefPm6bLLLjvhsjabTTab7ZTrCwAATg9+67kJDg5Wly5dlJ6e7lOenp6unj17Hne9Dz/8UCNHjtQHH3ygK6+8sq6rCQAATjN+67mRpHHjxunWW29V165d1aNHD7311lvKyMjQmDFjJHkOKf3555969913JXmCzfDhw/V///d/uvDCC729PqGhoYqOjvbb5wAAAA2HX8PNkCFDlJOTo0mTJikzM1MdOnTQokWLlJqaKknKzMz0uebNm2++qdLSUt199926++67veUjRozQrFmz6rv6AACgAfLrdW78gevcAABw+jktrnMDAABQFwg3AAAgoBBuAABAQCHcAACAgEK4AQAAAYVwAwAAAgrhBgAABBTCDQAACCiEGwAAEFAINwAAIKAQbgAAQEAh3AAAgIBCuAEAAAHF6u8KNESGYai0tFQul8vfVTnjWSwWWa1WmUwmf1cFAHCaINwcw+FwKDMzU0VFRf6uCsqEhYUpOTlZwcHB/q4KAOA0QLg5itvt1o4dO2SxWJSSkqLg4GB6DPzIMAw5HA4dOHBAO3bsUKtWrWQ2cyQVAHBihJujOBwOud1uNWvWTGFhYf6uDiSFhoYqKChIu3btksPhUEhIiL+rBABo4PgaXAl6BxoWfh4AgOpgrwEAAAIK4QYAAAQUwg0AAAgohBsAABBQCDcB5Msvv9TFF1+smJgYxcXF6aqrrtIff/zhnb9nzx4NHTpUsbGxCg8PV9euXfXjjz965y9YsEBdu3ZVSEiI4uPjdd111/njYwAAcEo4FfwkDMNQsdM/VyoODbJU6zo7hYWFGjdunM4991wVFhbqySef1LXXXqsNGzaoqKhIvXv3VpMmTbRgwQIlJSVp3bp1crvdkqSFCxfquuuu04QJE/Tee+/J4XBo4cKFdfXRAACoMybDMAx/V6I+5eXlKTo6Wrm5uYqKivKZV1JSoh07digtLc17PZUiR6naPfmVP6qqLZMuV1hwzfPngQMHlJCQoE2bNun777/XQw89pJ07dyo2NrbCsj179lTLli31/vvvn0qV60RlPxcAwJnlRPvvY3FYKoD88ccfuvnmm9WyZUtFRUUpLS1NkpSRkaENGzbo/PPPrzTYSNKGDRvUt2/f+qwuAAB1gsNSJxEaZNGWSZf77b2rY9CgQWrWrJnefvttpaSkyO12q0OHDnI4HAoNDT3xe51kPgAApwvCzUmYTKZTOjRUX3JycrR161a9+eab6tWrlyTp22+/9c7v2LGjpk2bpoMHD1bae9OxY0d98803GjVqVL3VGQCAusBhqQDRqFEjxcXF6a233tLvv/+uJUuWaNy4cd75N910k5KSkjR48GB999132r59uz7++GOtWrVKkjRx4kR9+OGHmjhxorZu3apNmzbphRde8NfHAQCgxgg3AcJsNmvOnDlau3atOnTooAceeED//Oc/vfODg4O1ePFiJSQkaODAgTr33HP13HPPyWLxHPq65JJLNG/ePC1YsEDnnXee+vTp43OaOAAApwvOljoKZ+U0TPxcAACcLQUAAM5YhBsAABBQCDcAACCgEG4AAEBAIdwAAICAQrgBAAABhXADAAACCuEGAAAEFMINAAAIKIQbSJJatGihV155xd/VAADglBFuAABAQCHcAACAgEK4CQBvvvmmmjRpIrfb7VN+9dVXa8SIEfrjjz90zTXXKDExUREREbrgggv09ddf1/j9XnrpJZ177rkKDw9Xs2bNdNddd6mgoMBnme+++069e/dWWFiYGjVqpMsvv1yHDh2SJLndbj3//PM6++yzZbPZ1Lx5c/3v//5vjesDAMDRCDcnYxiSo9A/UxVv2P7Xv/5V2dnZWrp0qbfs0KFD+uqrrzRs2DAVFBRo4MCB+vrrr7V+/XpdfvnlGjRokDIyMmrUJGazWa+++qp++eUXvfPOO1qyZIkefvhh7/wNGzaob9++at++vVatWqVvv/1WgwYNksvlkiSNHz9ezz//vJ544glt2bJFH3zwgRITE2tUFwAAjmUyjCruQQPEiW6ZXlJSoh07digtLU0hISGeQkeh9I8UP9RU0mN7peDwKi16zTXXKD4+XtOnT5ckvfXWW5o4caL27Nkji8VSYfn27dvrzjvv1D333CPJM6B47NixGjt2bLWrOW/ePN15553Kzs6WJN18883KyMjQt99+W2HZ/Px8NW7cWK+//rpuv/32Km2/0p8LAOCMcqL997HouQkQw4YN08cffyy73S5Jmj17toYOHSqLxaLCwkI9/PDDateunWJiYhQREaH//ve/Ne65Wbp0qfr166cmTZooMjJSw4cPV05OjgoLCyUd6bmpzNatW2W32487HwCAU2X1dwUavKAwTw+Kv967igYNGiS3262FCxfqggsu0MqVK/XSSy9Jkv7+97/rq6++0osvvqizzz5boaGhuuGGG+RwOKpdpV27dmngwIEaM2aMnnnmGcXGxurbb7/V6NGj5XQ6JUmhoaHHXf9E8wAAqA2Em5Mxmap8aMifQkNDdd1112n27Nn6/fff1bp1a3Xp0kWStHLlSo0cOVLXXnutJKmgoEA7d+6s0fusWbNGpaWl+te//iWz2dPx99FHH/ks07FjR33zzTd6+umnK6zfqlUrhYaG6ptvvqnyYSkAAKqDcBNAhg0bpkGDBmnz5s265ZZbvOVnn322PvnkEw0aNEgmk0lPPPFEhTOrquqss85SaWmpXnvtNQ0aNEjfffed3njjDZ9lxo8fr3PPPVd33XWXxowZo+DgYC1dulR//etfFR8fr0ceeUQPP/ywgoODddFFF+nAgQPavHmzRo8efUqfHwAAiTE3AaVPnz6KjY3Vtm3bdPPNN3vLX375ZTVq1Eg9e/bUoEGDdPnll6tz5841eo/zzjtPL730kp5//nl16NBBs2fP1uTJk32Wad26tRYvXqyNGzeqW7du6tGjhz777DNZrZ4s/cQTT+jBBx/Uk08+qbZt22rIkCHKysqq+QcHAOAonC11FM7KaZj4uQAAOFsKAACcsQg38DF79mxFRERUOrVv397f1QMA4KQYUAwfV199tbp3717pvKCgoHquDQAA1Ue4gY/IyEhFRkb6uxoAANQYh6UAAEBAIdxU4gw7gazB4+cBAKgOws1RyseUFBUV+bkmOFr5z4MxPwCAqmDMzVEsFotiYmK8F5QLCwuTyWTyc63OXIZhqKioSFlZWYqJian07uYAAByLcHOMpKQkSeKKuQ1ITEyM9+cCAMDJEG6OYTKZlJycrISEBO9druE/QUFB9NgAAKrF7+FmypQp+uc//6nMzEy1b99er7zyinr16nXc5ZcvX65x48Zp8+bNSklJ0cMPP6wxY8bUer0sFgs7VQAATkN+HVA8d+5cjR07VhMmTND69evVq1cvDRgwQBkZGZUuv2PHDg0cOFC9evXS+vXr9dhjj+m+++7Txx9/XM81BwAADZVfb5zZvXt3de7cWVOnTvWWtW3bVoMHD65wp2lJeuSRR7RgwQJt3brVWzZmzBht3LhRq1atqtJ7VufGWwAAoGE4LW6c6XA4tHbtWvXv39+nvH///vr+++8rXWfVqlUVlr/88su1Zs0axscAAABJfhxzk52dLZfLpcTERJ/yxMRE7du3r9J19u3bV+nypaWlys7OVnJycoV17Ha77Ha793Vubq4kTwIEAACnh/L9dlUOOPl9QPGx15ExDOOE15apbPnKystNnjxZTz/9dIXyZs2aVbeqAADAz/Lz8xUdHX3CZfwWbuLj42WxWCr00mRlZVXonSmXlJRU6fJWq1VxcXGVrjN+/HiNGzfO+9rtduvgwYOKi4ur1gX68vLy1KxZM+3evZuxOvWA9q5ftHf9or3rF+1dv+qqvQ3DUH5+vlJSUk66rN/CTXBwsLp06aL09HRde+213vL09HRdc801la7To0cP/b//9/98yhYvXqyuXbse99L8NptNNpvNpywmJqbG9Y6KiuKPox7R3vWL9q5ftHf9or3rV12098l6bMr59VTwcePGadq0aZoxY4a2bt2qBx54QBkZGd7r1owfP17Dhw/3Lj9mzBjt2rVL48aN09atWzVjxgxNnz5dDz30kL8+AgAAaGD8OuZmyJAhysnJ0aRJk5SZmakOHTpo0aJFSk1NlSRlZmb6XPMmLS1NixYt0gMPPKB///vfSklJ0auvvqrrr7/eXx8BAAA0MH4fUHzXXXfprrvuqnTerFmzKpT17t1b69atq+NaVWSz2TRx4sQKh7hQN2jv+kV71y/au37R3vWrIbS3Xy/iBwAAUNv8OuYGAACgthFuAABAQCHcAACAgEK4AQAAAYVwUwVTpkxRWlqaQkJC1KVLF61cudLfVQoIK1as0KBBg5SSkiKTyaRPP/3UZ75hGHrqqaeUkpKi0NBQXXLJJdq8ebN/KhsAJk+erAsuuECRkZFKSEjQ4MGDtW3bNp9laPPaM3XqVHXs2NF7IbMePXroiy++8M6nrevW5MmTZTKZNHbsWG8ZbV57nnrqKZlMJp8pKSnJO9/fbU24OYm5c+dq7NixmjBhgtavX69evXppwIABPtffQc0UFhaqU6dOev311yud/8ILL+ill17S66+/rtWrVyspKUn9+vVTfn5+Pdc0MCxfvlx33323fvjhB6Wnp6u0tFT9+/dXYWGhdxnavPY0bdpUzz33nNasWaM1a9aoT58+uuaaa7z/4GnrurN69Wq99dZb6tixo085bV672rdvr8zMTO+0adMm7zy/t7WBE+rWrZsxZswYn7JzzjnHePTRR/1Uo8AkyZg/f773tdvtNpKSkoznnnvOW1ZSUmJER0cbb7zxhh9qGHiysrIMScby5csNw6DN60OjRo2MadOm0dZ1KD8/32jVqpWRnp5u9O7d27j//vsNw+D3u7ZNnDjR6NSpU6XzGkJb03NzAg6HQ2vXrlX//v19yvv376/vv//eT7U6M+zYsUP79u3zaXubzabevXvT9rUkNzdXkhQbGyuJNq9LLpdLc+bMUWFhoXr06EFb16G7775bV155pS677DKfctq89v32229KSUlRWlqahg4dqu3bt0tqGG3t9ysUN2TZ2dlyuVwV7lKemJhY4e7kqF3l7VtZ2+/atcsfVQoohmFo3Lhxuvjii9WhQwdJtHld2LRpk3r06KGSkhJFRERo/vz5ateunfcfPG1du+bMmaN169Zp9erVFebx+127unfvrnfffVetW7fW/v379eyzz6pnz57avHlzg2hrwk0VmEwmn9eGYVQoQ92g7evGPffco59//lnffvtthXm0ee1p06aNNmzYoMOHD+vjjz/WiBEjtHz5cu982rr27N69W/fff78WL16skJCQ4y5Hm9eOAQMGeJ+fe+656tGjh8466yy98847uvDCCyX5t605LHUC8fHxslgsFXppsrKyKiRS1K7yUfe0fe279957tWDBAi1dulRNmzb1ltPmtS84OFhnn322unbtqsmTJ6tTp076v//7P9q6Dqxdu1ZZWVnq0qWLrFarrFarli9frldffVVWq9XbrrR53QgPD9e5556r3377rUH8fhNuTiA4OFhdunRRenq6T3l6erp69uzpp1qdGdLS0pSUlOTT9g6HQ8uXL6fta8gwDN1zzz365JNPtGTJEqWlpfnMp83rnmEYstvttHUd6Nu3rzZt2qQNGzZ4p65du2rYsGHasGGDWrZsSZvXIbvdrq1btyo5Oblh/H7Xy7Dl09icOXOMoKAgY/r06caWLVuMsWPHGuHh4cbOnTv9XbXTXn5+vrF+/Xpj/fr1hiTjpZdeMtavX2/s2rXLMAzDeO6554zo6Gjjk08+MTZt2mTcdNNNRnJyspGXl+fnmp+e7rzzTiM6OtpYtmyZkZmZ6Z2Kioq8y9DmtWf8+PHGihUrjB07dhg///yz8dhjjxlms9lYvHixYRi0dX04+mwpw6DNa9ODDz5oLFu2zNi+fbvxww8/GFdddZURGRnp3Tf6u60JN1Xw73//20hNTTWCg4ONzp07e0+dxalZunSpIanCNGLECMMwPKcTTpw40UhKSjJsNpvxl7/8xdi0aZN/K30aq6ytJRkzZ870LkOb157bbrvN+3+jcePGRt++fb3BxjBo6/pwbLihzWvPkCFDjOTkZCMoKMhISUkxrrvuOmPz5s3e+f5ua5NhGEb99BEBAADUPcbcAACAgEK4AQAAAYVwAwAAAgrhBgAABBTCDQAACCiEGwAAEFAINwAAIKAQbgCckUwmkz799FN/VwNAHSDcAKh3I0eOlMlkqjBdccUV/q4agABg9XcFAJyZrrjiCs2cOdOnzGaz+ak2AAIJPTcA/MJmsykpKclnatSokSTPIaOpU6dqwIABCg0NVVpamubNm+ez/qZNm9SnTx+FhoYqLi5Of/vb31RQUOCzzIwZM9S+fXvZbDYlJyfrnnvu8ZmfnZ2ta6+9VmFhYWrVqpUWLFjgnXfo0CENGzZMjRs3VmhoqFq1alUhjAFomAg3ABqkJ554Qtdff702btyoW265RTfddJO2bt0qSSoqKtIVV1yhRo0aafXq1Zo3b56+/vprn/AydepU3X333frb3/6mTZs2acGCBTr77LN93uPpp5/WjTfeqJ9//lkDBw7UsGHDdPDgQe/7b9myRV988YW2bt2qqVOnKj4+vv4aAEDN1dstOgGgzIgRIwyLxWKEh4f7TJMmTTIMw3MH8zFjxvis0717d+POO+80DMMw3nrrLaNRo0ZGQUGBd/7ChQsNs9ls7Nu3zzAMw0hJSTEmTJhw3DpIMh5//HHv64KCAsNkMhlffPGFYRiGMWjQIGPUqFG184EB1CvG3ADwi0svvVRTp071KYuNjfU+79Gjh8+8Hj16aMOGDZKkrVu3qlOnTgoPD/fOv+iii+R2u7Vt2zaZTCbt3btXffv2PWEdOnbs6H0eHh6uyMhIZWVlSZLuvPNOXX/99Vq3bp369++vwYMHq2fPnjX6rADqF+EGgF+Eh4dXOEx0MiaTSZJkGIb3eWXLhIaGVml7QUFBFdZ1u92SpAEDBmjXrl1auHChvv76a/Xt21d33323XnzxxWrVGUD9Y8wNgAbphx9+qPD6nHPOkSS1a9dOGzZsUGFhoXf+d999J7PZrNatWysyMlItWrTQN998c0p1aNy4sUaOHKn3339fr7zyit56661T2h6A+kHPDQC/sNvt2rdvn0+Z1Wr1DtqdN2+eunbtqosvvlizZ8/WTz/9pOnTp0uShg0bpokTJ2rEiBF66qmndODAAd1777269dZblZiYKEl66qmnNGbMGCUkJGjAgAHKz8/Xd999p3vvvbdK9XvyySfVpUsXtW/fXna7XZ9//rnatm1biy0AoK4QbgD4xZdffqnk5GSfsjZt2ui///2vJM+ZTHPmzNFdd92lpKQkzZ49W+3atZMkhYWF6auvvtL999+vCy64QGFhYbr++uv10ksvebc1YsQIlZSU6OWXX9ZDDz2k+Ph43XDDDVWuX3BwsMaPH6+dO3cqNDRUvXr10pw5c2rhkwOoaybDMAx/VwIAjmYymTR//nwNHjzY31UBcBpizA0AAAgohBsAABBQGHMDoMHhaDmAU0HPDQAACCiEGwAAEFAINwAAIKAQbgAAQEAh3AAAgIBCuAEAAAGFcAMAAAIK4QYAAAQUwg0AAAgo/x93TZx0jpaEmgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== 7) Plot =====\n",
    "# acc = history.history.get(\"accuracy\", []); val_acc = history.history.get(\"val_accuracy\", [])\n",
    "# plt.figure(); plt.plot(range(1,len(acc)+1), acc, label=\"acc\"); plt.plot(range(1,len(val_acc)+1), val_acc, label=\"val_acc\")\n",
    "# plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Training vs Validation\"); plt.show()\n",
    "\n",
    "acc = history.history.get(\"accuracy\", [])\n",
    "val_acc = history.history.get(\"val_accuracy\", [])\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, label=\"acc\")\n",
    "plt.plot(epochs, val_acc, label=\"val_acc\")\n",
    "plt.ylim(0, 1)                         # 🔁 변경점 4: 축 고정으로 왜곡 방지\n",
    "plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f5ba35d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpranic59x/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpranic59x/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpranic59x'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 79), dtype=tf.float32, name='features')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  13383636304: TensorSpec(shape=(1, 79), dtype=tf.float32, name=None)\n",
      "  13383644560: TensorSpec(shape=(1, 79), dtype=tf.float32, name=None)\n",
      "  13383642640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13383642832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13383633616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13383632464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13383640720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13383646480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "TFLite 모델이 저장되었습니다: /Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/phishing_model.tflite\n",
      "피처 정보가 저장되었습니다: /Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/feature_info.json\n",
      "TFLite 모델이 저장되었습니다: /Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/phishing_model.tflite\n",
      "피처 정보가 저장되었습니다: /Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/feature_info.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1763743083.090194 4171299 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1763743083.090206 4171299 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-11-22 01:38:03.090296: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpranic59x\n",
      "2025-11-22 01:38:03.090529: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-11-22 01:38:03.090533: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpranic59x\n",
      "2025-11-22 01:38:03.093223: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-11-22 01:38:03.108580: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpranic59x\n",
      "2025-11-22 01:38:03.113224: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 22928 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# ===== 8) TFLite 변환 =====\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# TFLite 모델 저장\n",
    "# Save outputs into the repo-local phishing/ folder\n",
    "# Use absolute paths for outputs to ensure we write into the repo phishing/ folder\n",
    "tflite_path = \"/Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/phishing_model.tflite\"\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"TFLite 모델이 저장되었습니다: {tflite_path}\")\n",
    "\n",
    "# 피처 정보 저장 (Android에서 사용)\n",
    "import json\n",
    "# Export feature_info with labeling metadata so the app can handle soft labels & uncertainty\n",
    "feature_info = {\n",
    "    \"feature_columns\": numeric_cols,\n",
    "    \"input_shape\": [X_train.shape[1]],\n",
    "    \"normalization_layer\": \"norm_all\",\n",
    "    \"labeling\": LABEL_MODE,  # 'soft'|'binary'|'both'\n",
    "    \"uncertainty_notify_eps\": 0.05  # app should flag predictions in [0.5-eps, 0.5+eps] as low confidence\n",
    "}\n",
    "\n",
    "feature_info_path = \"/Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/feature_info.json\"\n",
    "with open(feature_info_path, 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "print(f\"피처 정보가 저장되었습니다: {feature_info_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
