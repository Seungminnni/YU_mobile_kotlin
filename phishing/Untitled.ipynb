{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd8263d4-5059-4d8d-b6a8-3f66a35bf38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Reset & Imports =====\n",
    "# 환경 설정: tf_env 사용\n",
    "import os\n",
    "os.environ['CONDA_DEFAULT_ENV'] = 'tf_env'\n",
    "\n",
    "import gc, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b90520d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 경로: /opt/anaconda3/envs/tf_env/bin/python\n",
      "Python 버전: 3.11.11 | packaged by conda-forge | (main, Dec  5 2024, 08:47:03) [Clang 18.1.8 ]\n",
      "TensorFlow 버전: 2.18.0\n",
      "\n",
      "설치된 패키지 일부:\n",
      "\n",
      "설치된 패키지 일부:\n"
     ]
    }
   ],
   "source": [
    "# Python 환경 확인\n",
    "import sys\n",
    "print(\"Python 경로:\", sys.executable)\n",
    "print(\"Python 버전:\", sys.version)\n",
    "\n",
    "# 설치된 패키지 확인\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(\"TensorFlow 버전:\", tf.__version__)\n",
    "except ImportError as e:\n",
    "    print(\"TensorFlow 임포트 실패:\", e)\n",
    "    \n",
    "# pip 경로 확인\n",
    "import subprocess\n",
    "result = subprocess.run([sys.executable, '-m', 'pip', 'list'], capture_output=True, text=True)\n",
    "print(\"\\n설치된 패키지 일부:\")\n",
    "for line in result.stdout.split('\\n')[:10]:\n",
    "    if 'tensor' in line.lower():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a032a0ab-7fd3-4252-882f-02d1412a8ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전 초기화\n",
    "try: del model\n",
    "except: pass\n",
    "gc.collect()\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42); tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2755837-2f19-4330-9385-36cf47073f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1) Config =====\n",
    "# Use the repo-local TFLite-ready CSV so feature names/order match mobile assets\n",
    "# Use absolute paths so notebook runs reliably regardless of working directory\n",
    "# Set these to your repo path — change if your project is elsewhere on disk\n",
    "DATA_PATH = \"/Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/phishing_data_tflite_ready.csv\"\n",
    "TARGET = \"status\"\n",
    "BATCH_SIZE, EPOCHS = 64, 100\n",
    "# LABEL_MODE selects training target: 'soft' uses probability labels, 'binary' uses 0/1, 'both' keeps both and defaults to soft training\n",
    "LABEL_MODE = 'both'  # options: 'soft' | 'binary' | 'both'\n",
    "# APPLY_SAMPLE_WEIGHT: when True we'll down-weight samples with labels near 0.5 (uncertain).\n",
    "# If you want to preserve 0.5 exactly and not reduce its influence, set this to False\n",
    "APPLY_SAMPLE_WEIGHT = False #필요에 따라 수정\n",
    "EARLY_STOP_AT_ACC = 0.98\n",
    "# LABEL_NA_POLICY controls how rows with missing/invalid label_soft are handled\n",
    "# options: 'drop' (remove rows), 'impute' (fill with median), 'raise' (error out)\n",
    "LABEL_NA_POLICY = 'drop'\n",
    "# If using 'impute', this value will be used (median recommended)\n",
    "LABEL_NA_IMPUTE_VALUE = None\n",
    "# ===== 2) Load & Clean =====\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "if \"url\" in df.columns: df = df.drop(columns=[\"url\"])\n",
    "df = shuffle(df, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68322c57-07f9-4074-8e65-287c1a7e6f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_soft NaNs: 0\n",
      "LABEL_NA_POLICY = drop\n",
      "label_soft min/max/mean: 0.0 1.0 0.5000436\n",
      "Label counts (soft approx) top: {1.0: 5741, 0.0: 5740}\n",
      "Label counts (binary): {1.0: 5741, 0.0: 5740}\n"
     ]
    }
   ],
   "source": [
    "# 라벨 정리 (soft-first 안전 처리) — soft 라벨을 우선 보존합니다\n",
    "# 1) 표준화: 문자열 정리 및 명시적 매핑\n",
    "tmp = df[TARGET].astype(str).str.strip().str.lower()\n",
    "mapping = {'legitimate': '0', 'phishing': '1', 'zero': '0', 'one': '1'}\n",
    "tmp = tmp.replace(mapping)\n",
    "# 2) 숫자로 안전하게 변환 — invalid는 NaN으로 처리\n",
    "df['label_soft'] = pd.to_numeric(tmp, errors='coerce').astype('float32')\n",
    "# 3) NaN / 범위 검사\n",
    "n_missing = int(df['label_soft'].isna().sum())\n",
    "print('label_soft NaNs:', n_missing)\n",
    "if 'LABEL_NA_POLICY' in globals():\n",
    "    print('LABEL_NA_POLICY =', LABEL_NA_POLICY)\n",
    "if n_missing > 0:\n",
    "    if LABEL_NA_POLICY == 'drop':\n",
    "        print(f'Dropping {n_missing} rows with missing labels')\n",
    "        df = df.dropna(subset=['label_soft']).reset_index(drop=True)\n",
    "    elif LABEL_NA_POLICY == 'impute':\n",
    "        if LABEL_NA_IMPUTE_VALUE is None:\n",
    "            fill = float(df['label_soft'].median())\n",
    "        else:\n",
    "            fill = float(LABEL_NA_IMPUTE_VALUE)\n",
    "        print('Imputing missing labels with', fill)\n",
    "        df['label_soft'] = df['label_soft'].fillna(fill)\n",
    "    elif LABEL_NA_POLICY == 'raise':\n",
    "        raise ValueError('Found missing labels (label_soft) and LABEL_NA_POLICY==raise')\n",
    "# 4) Ensure label range is [0,1] — clip with warning if out of bounds\n",
    "minv, maxv = df['label_soft'].min(), df['label_soft'].max()\n",
    "if minv < 0 or maxv > 1:\n",
    "    print('Warning: label_soft out of [0,1] range — clipping')\n",
    "    df['label_soft'] = df['label_soft'].clip(0.0, 1.0)\n",
    "print('label_soft min/max/mean:', df['label_soft'].min(), df['label_soft'].max(), df['label_soft'].mean())\n",
    "# 5) hard binary label 생성 (앱/배포 호환성 용도)\n",
    "df['label_binary'] = (df['label_soft'] >= 0.5).astype('float32')\n",
    "print('Label counts (soft approx) top:', df['label_soft'].value_counts().head(10).to_dict())\n",
    "print('Label counts (binary):', df['label_binary'].value_counts().to_dict())\n",
    "# Note: we keep both label_soft and label_binary in the dataframe; training target selection controlled by LABEL_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7006c5fe-98b5-4ecb-8c0a-228c61b6cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3) 숫자 피처만 안전하게 선택 =====\n",
    "# Exclude the target and any internal label columns we created earlier (label_soft, label_binary)\n",
    "exclude_cols = {TARGET, 'label_soft', 'label_binary'}\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "numeric_cols = []\n",
    "for c in feature_cols:\n",
    "    ser = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if ser.notna().any():           # 전부 NaN이 아니면 사용\n",
    "        df[c] = ser\n",
    "        numeric_cols.append(c)\n",
    "\n",
    "assert len(numeric_cols) > 0, \"사용 가능한 숫자 피처가 없습니다.\"\n",
    "# # --- Debugging output: show exactly which feature columns were candidates and which numeric columns were selected\n",
    "# print('\\n=== Feature selection summary ===')\n",
    "# print('total df columns:', len(df.columns))\n",
    "# # show excluded columns, and whether they are present in the current dataframe\n",
    "# print('excluded columns (target + internal labels):', sorted(list(exclude_cols)))\n",
    "# present_excluded = [c for c in exclude_cols if c in df.columns]\n",
    "# missing_excluded = [c for c in exclude_cols if c not in df.columns]\n",
    "# print('excluded_present_in_df:', present_excluded)\n",
    "# print('excluded_missing_in_df:', missing_excluded)\n",
    "# print('feature candidate columns (feature_cols) count:', len(feature_cols))\n",
    "# print('feature candidate preview (first 20):', feature_cols[:20])\n",
    "# print('numeric columns selected (numeric_cols) count:', len(numeric_cols))\n",
    "# print('numeric columns selected:', numeric_cols)\n",
    "# label_in_numeric = [c for c in numeric_cols if c in (TARGET, 'label_soft', 'label_binary')]\n",
    "# if label_in_numeric:\n",
    "#     print('WARNING: numeric_cols unexpectedly contains label-like columns:', label_in_numeric)\n",
    "# else:\n",
    "#     print('OK: no internal label columns are present in numeric_cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbc18bd6-4637-4da0-8aa1-a8ec0a6b12a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (9184, 79)  | X_val: (2297, 79)  | n_features: 79\n"
     ]
    }
   ],
   "source": [
    "# 결측치 중앙값으로 채우기\n",
    "for c in numeric_cols:\n",
    "    med = float(df[c].dropna().median()) if df[c].notna().any() else 0.0\n",
    "    df[c] = df[c].fillna(med).astype(\"float32\")\n",
    "\n",
    "# ===== 4) 행렬(X, y)로 변환 =====\n",
    "X = df[numeric_cols].to_numpy(dtype=\"float32\")\n",
    "# Prepare soft and binary targets — do not overwrite original soft values\n",
    "y_soft = df['label_soft'].to_numpy(dtype=\"float32\")\n",
    "y_binary = df['label_binary'].to_numpy(dtype=\"float32\")\n",
    "# Choose training target based on LABEL_MODE (soft | binary | both). 'both' will train using soft by default\n",
    "if LABEL_MODE == 'soft':\n",
    "    y = y_soft\n",
    "elif LABEL_MODE == 'binary':\n",
    "    y = y_binary\n",
    "else:  # 'both' default to soft for training but keep binary for evaluation/export\n",
    "    y = y_soft\n",
    "\n",
    "\n",
    "# Use index-based split so soft and binary targets align with the same train/validation indices\n",
    "# Ensure no NaNs in the stratify label (y_binary). If any appear, drop those rows first\n",
    "mask_valid = ~np.isnan(y_binary)\n",
    "if not mask_valid.all():\n",
    "    n_bad = int((~mask_valid).sum())\n",
    "    print(f'Warning: {n_bad} rows with NaN binary label found. Dropping before split.')\n",
    "    X = X[mask_valid]\n",
    "    y_soft = y_soft[mask_valid]\n",
    "    y_binary = y_binary[mask_valid]\n",
    "indices = np.arange(X.shape[0])\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=y_binary, random_state=42)\n",
    "X_train, X_val = X[train_idx], X[val_idx]\n",
    "# aligned splits for both target types\n",
    "y_soft_train, y_soft_val = y_soft[train_idx], y_soft[val_idx]\n",
    "y_binary_train, y_binary_val = y_binary[train_idx], y_binary[val_idx]\n",
    "if LABEL_MODE == 'soft':\n",
    "    y_train, y_val = y_soft_train, y_soft_val\n",
    "elif LABEL_MODE == 'binary':\n",
    "    y_train, y_val = y_binary_train, y_binary_val\n",
    "else:  # both - default to soft training, keep binary for evaluation\n",
    "    y_train, y_val = y_soft_train, y_soft_val\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \" | X_val:\", X_val.shape, \" | n_features:\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7beee866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train checks — shapes & metadata\n",
      "X_train shape: (9184, 79)\n",
      "X_val shape: (2297, 79)\n",
      "n_features (numeric_cols): 79\n",
      "Found ../app/src/main/assets/feature_info.json: input_shape=[79], feature_columns=79\n",
      "y_soft_train min/max/mean: 0.0 1.0 0.5\n",
      "y_binary_train counts: (array([0., 1.], dtype=float32), array([4592, 4592]))\n",
      "Pre-train checks done — proceed to model creation if OK\n"
     ]
    }
   ],
   "source": [
    "# ===== PRE-TRAIN CHECKS =====\n",
    "import os, json, numpy as np\n",
    "print('Pre-train checks — shapes & metadata')\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('n_features (numeric_cols):', len(numeric_cols))\n",
    "if X_train.shape[1] != len(numeric_cols):\n",
    "    raise AssertionError(f'ERROR: feature column count mismatch: X_train has {X_train.shape[1]} cols but numeric_cols lists {len(numeric_cols)}')\n",
    "# Compare to existing feature_info in repo/app assets if present\n",
    "fi_paths = ['phishing/feature_info.json', 'app/src/main/assets/feature_info.json', '../app/src/main/assets/feature_info.json']\n",
    "for p in fi_paths:\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            with open(p) as f:\n",
    "                fi = json.load(f)\n",
    "            input_shape = fi.get('input_shape')\n",
    "            feat_cols = fi.get('feature_columns', [])\n",
    "            print(f'Found {p}: input_shape={input_shape}, feature_columns={len(feat_cols)}')\n",
    "            if isinstance(input_shape, (list, tuple)) and len(input_shape) > 0 and input_shape[0] != X_train.shape[1]:\n",
    "                print(f'WARNING: {p} input_shape[0] ({input_shape[0]}) != n_features ({X_train.shape[1]})')\n",
    "        except Exception as e:\n",
    "            print('Could not read', p, e)\n",
    "# basic label distribution checks\n",
    "print('y_soft_train min/max/mean:', np.min(y_soft_train), np.max(y_soft_train), np.mean(y_soft_train))\n",
    "print('y_binary_train counts:', np.unique(y_binary_train, return_counts=True))\n",
    "# sample_weight sanity if enabled (applies when training on soft labels)\n",
    "if APPLY_SAMPLE_WEIGHT and LABEL_MODE in ('soft', 'both'):\n",
    "    sw = np.clip(np.abs(y_train - 0.5) * 2.0, 0.1, 1.0)\n",
    "    print('sample_weight min/max/mean:', sw.min(), sw.max(), sw.mean())\n",
    "print('Pre-train checks done — proceed to model creation if OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efbeb252-e2ed-4e31-b0d7-0feb6df67368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"phish_numeric_only\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"phish_numeric_only\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ features (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ norm_all (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">79</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">159</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ features (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ norm_all (\u001b[38;5;33mNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m79\u001b[0m)             │           \u001b[38;5;34m159\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m5,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,392</span> (28.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,392\u001b[0m (28.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,233</span> (28.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,233\u001b[0m (28.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">159</span> (640.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m159\u001b[0m (640.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== 5) 모델 (단일 입력) =====\n",
    "inp = keras.Input(shape=(X_train.shape[1],), name=\"features\", dtype=tf.float32)\n",
    "norm = layers.Normalization(name=\"norm_all\")\n",
    "norm.adapt(X_train)\n",
    "\n",
    "x = norm(inp)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inp, outputs=out, name=\"phish_numeric_only\")\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0992483d-0794-444d-9f7d-b45ad29ba174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8248 - loss: 0.3887 - val_accuracy: 0.8903 - val_loss: 0.2731\n",
      "Epoch 2/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8248 - loss: 0.3887 - val_accuracy: 0.8903 - val_loss: 0.2731\n",
      "Epoch 2/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - accuracy: 0.9007 - loss: 0.2517 - val_accuracy: 0.9020 - val_loss: 0.2371\n",
      "Epoch 3/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - accuracy: 0.9007 - loss: 0.2517 - val_accuracy: 0.9020 - val_loss: 0.2371\n",
      "Epoch 3/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - accuracy: 0.9160 - loss: 0.2215 - val_accuracy: 0.9164 - val_loss: 0.2202\n",
      "Epoch 4/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - accuracy: 0.9160 - loss: 0.2215 - val_accuracy: 0.9164 - val_loss: 0.2202\n",
      "Epoch 4/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - accuracy: 0.9211 - loss: 0.2037 - val_accuracy: 0.9199 - val_loss: 0.2081\n",
      "Epoch 5/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - accuracy: 0.9211 - loss: 0.2037 - val_accuracy: 0.9199 - val_loss: 0.2081\n",
      "Epoch 5/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - accuracy: 0.9236 - loss: 0.1928 - val_accuracy: 0.9264 - val_loss: 0.2012\n",
      "Epoch 6/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - accuracy: 0.9236 - loss: 0.1928 - val_accuracy: 0.9264 - val_loss: 0.2012\n",
      "Epoch 6/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - accuracy: 0.9288 - loss: 0.1816 - val_accuracy: 0.9282 - val_loss: 0.1928\n",
      "Epoch 7/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - accuracy: 0.9288 - loss: 0.1816 - val_accuracy: 0.9282 - val_loss: 0.1928\n",
      "Epoch 7/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - accuracy: 0.9334 - loss: 0.1718 - val_accuracy: 0.9277 - val_loss: 0.1861\n",
      "Epoch 8/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - accuracy: 0.9334 - loss: 0.1718 - val_accuracy: 0.9277 - val_loss: 0.1861\n",
      "Epoch 8/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - accuracy: 0.9361 - loss: 0.1654 - val_accuracy: 0.9295 - val_loss: 0.1844\n",
      "Epoch 9/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - accuracy: 0.9361 - loss: 0.1654 - val_accuracy: 0.9295 - val_loss: 0.1844\n",
      "Epoch 9/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - accuracy: 0.9422 - loss: 0.1575 - val_accuracy: 0.9308 - val_loss: 0.1779\n",
      "Epoch 10/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - accuracy: 0.9422 - loss: 0.1575 - val_accuracy: 0.9308 - val_loss: 0.1779\n",
      "Epoch 10/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - accuracy: 0.9411 - loss: 0.1536 - val_accuracy: 0.9330 - val_loss: 0.1737\n",
      "Epoch 11/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - accuracy: 0.9411 - loss: 0.1536 - val_accuracy: 0.9330 - val_loss: 0.1737\n",
      "Epoch 11/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - accuracy: 0.9439 - loss: 0.1488 - val_accuracy: 0.9364 - val_loss: 0.1683\n",
      "Epoch 12/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - accuracy: 0.9439 - loss: 0.1488 - val_accuracy: 0.9364 - val_loss: 0.1683\n",
      "Epoch 12/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - accuracy: 0.9465 - loss: 0.1434 - val_accuracy: 0.9377 - val_loss: 0.1662\n",
      "Epoch 13/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - accuracy: 0.9465 - loss: 0.1434 - val_accuracy: 0.9377 - val_loss: 0.1662\n",
      "Epoch 13/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - accuracy: 0.9482 - loss: 0.1355 - val_accuracy: 0.9434 - val_loss: 0.1656\n",
      "Epoch 14/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - accuracy: 0.9482 - loss: 0.1355 - val_accuracy: 0.9434 - val_loss: 0.1656\n",
      "Epoch 14/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - accuracy: 0.9505 - loss: 0.1334 - val_accuracy: 0.9443 - val_loss: 0.1632\n",
      "Epoch 15/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - accuracy: 0.9505 - loss: 0.1334 - val_accuracy: 0.9443 - val_loss: 0.1632\n",
      "Epoch 15/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - accuracy: 0.9485 - loss: 0.1323 - val_accuracy: 0.9434 - val_loss: 0.1593\n",
      "Epoch 16/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - accuracy: 0.9485 - loss: 0.1323 - val_accuracy: 0.9434 - val_loss: 0.1593\n",
      "Epoch 16/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - accuracy: 0.9530 - loss: 0.1272 - val_accuracy: 0.9434 - val_loss: 0.1557\n",
      "Epoch 17/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - accuracy: 0.9530 - loss: 0.1272 - val_accuracy: 0.9434 - val_loss: 0.1557\n",
      "Epoch 17/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - accuracy: 0.9525 - loss: 0.1249 - val_accuracy: 0.9451 - val_loss: 0.1542\n",
      "Epoch 18/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - accuracy: 0.9525 - loss: 0.1249 - val_accuracy: 0.9451 - val_loss: 0.1542\n",
      "Epoch 18/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - accuracy: 0.9520 - loss: 0.1228 - val_accuracy: 0.9421 - val_loss: 0.1545\n",
      "Epoch 19/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - accuracy: 0.9520 - loss: 0.1228 - val_accuracy: 0.9421 - val_loss: 0.1545\n",
      "Epoch 19/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - accuracy: 0.9551 - loss: 0.1173 - val_accuracy: 0.9438 - val_loss: 0.1523\n",
      "Epoch 20/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - accuracy: 0.9551 - loss: 0.1173 - val_accuracy: 0.9438 - val_loss: 0.1523\n",
      "Epoch 20/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - accuracy: 0.9632 - loss: 0.1053 - val_accuracy: 0.9460 - val_loss: 0.1529\n",
      "Epoch 21/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - accuracy: 0.9632 - loss: 0.1053 - val_accuracy: 0.9460 - val_loss: 0.1529\n",
      "Epoch 21/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - accuracy: 0.9579 - loss: 0.1097 - val_accuracy: 0.9478 - val_loss: 0.1499\n",
      "Epoch 22/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - accuracy: 0.9579 - loss: 0.1097 - val_accuracy: 0.9478 - val_loss: 0.1499\n",
      "Epoch 22/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - accuracy: 0.9604 - loss: 0.1067 - val_accuracy: 0.9491 - val_loss: 0.1488\n",
      "Epoch 23/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - accuracy: 0.9604 - loss: 0.1067 - val_accuracy: 0.9491 - val_loss: 0.1488\n",
      "Epoch 23/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - accuracy: 0.9612 - loss: 0.1032 - val_accuracy: 0.9517 - val_loss: 0.1458\n",
      "Epoch 24/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - accuracy: 0.9612 - loss: 0.1032 - val_accuracy: 0.9517 - val_loss: 0.1458\n",
      "Epoch 24/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - accuracy: 0.9636 - loss: 0.1030 - val_accuracy: 0.9504 - val_loss: 0.1452\n",
      "Epoch 25/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - accuracy: 0.9636 - loss: 0.1030 - val_accuracy: 0.9504 - val_loss: 0.1452\n",
      "Epoch 25/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - accuracy: 0.9628 - loss: 0.1010 - val_accuracy: 0.9495 - val_loss: 0.1454\n",
      "Epoch 26/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - accuracy: 0.9628 - loss: 0.1010 - val_accuracy: 0.9495 - val_loss: 0.1454\n",
      "Epoch 26/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - accuracy: 0.9644 - loss: 0.0977 - val_accuracy: 0.9504 - val_loss: 0.1451\n",
      "Epoch 27/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - accuracy: 0.9644 - loss: 0.0977 - val_accuracy: 0.9504 - val_loss: 0.1451\n",
      "Epoch 27/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - accuracy: 0.9665 - loss: 0.0932 - val_accuracy: 0.9499 - val_loss: 0.1451\n",
      "Epoch 28/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - accuracy: 0.9665 - loss: 0.0932 - val_accuracy: 0.9499 - val_loss: 0.1451\n",
      "Epoch 28/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - accuracy: 0.9639 - loss: 0.0982 - val_accuracy: 0.9552 - val_loss: 0.1423\n",
      "Epoch 29/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - accuracy: 0.9639 - loss: 0.0982 - val_accuracy: 0.9552 - val_loss: 0.1423\n",
      "Epoch 29/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - accuracy: 0.9668 - loss: 0.0892 - val_accuracy: 0.9530 - val_loss: 0.1456\n",
      "Epoch 30/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - accuracy: 0.9668 - loss: 0.0892 - val_accuracy: 0.9530 - val_loss: 0.1456\n",
      "Epoch 30/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - accuracy: 0.9679 - loss: 0.0904 - val_accuracy: 0.9525 - val_loss: 0.1432\n",
      "Epoch 31/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - accuracy: 0.9679 - loss: 0.0904 - val_accuracy: 0.9525 - val_loss: 0.1432\n",
      "Epoch 31/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - accuracy: 0.9684 - loss: 0.0880 - val_accuracy: 0.9521 - val_loss: 0.1443\n",
      "Epoch 32/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - accuracy: 0.9684 - loss: 0.0880 - val_accuracy: 0.9521 - val_loss: 0.1443\n",
      "Epoch 32/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - accuracy: 0.9714 - loss: 0.0810 - val_accuracy: 0.9543 - val_loss: 0.1384\n",
      "Epoch 33/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - accuracy: 0.9714 - loss: 0.0810 - val_accuracy: 0.9543 - val_loss: 0.1384\n",
      "Epoch 33/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - accuracy: 0.9714 - loss: 0.0814 - val_accuracy: 0.9517 - val_loss: 0.1424\n",
      "Epoch 34/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - accuracy: 0.9714 - loss: 0.0814 - val_accuracy: 0.9517 - val_loss: 0.1424\n",
      "Epoch 34/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - accuracy: 0.9707 - loss: 0.0840 - val_accuracy: 0.9530 - val_loss: 0.1414\n",
      "Epoch 35/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - accuracy: 0.9707 - loss: 0.0840 - val_accuracy: 0.9530 - val_loss: 0.1414\n",
      "Epoch 35/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - accuracy: 0.9733 - loss: 0.0753 - val_accuracy: 0.9556 - val_loss: 0.1412\n",
      "Epoch 36/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - accuracy: 0.9733 - loss: 0.0753 - val_accuracy: 0.9556 - val_loss: 0.1412\n",
      "Epoch 36/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - accuracy: 0.9709 - loss: 0.0814 - val_accuracy: 0.9547 - val_loss: 0.1419\n",
      "Epoch 37/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - accuracy: 0.9709 - loss: 0.0814 - val_accuracy: 0.9547 - val_loss: 0.1419\n",
      "Epoch 37/100\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - accuracy: 0.9707 - loss: 0.0763 - val_accuracy: 0.9539 - val_loss: 0.1470\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - accuracy: 0.9707 - loss: 0.0763 - val_accuracy: 0.9539 - val_loss: 0.1470\n"
     ]
    }
   ],
   "source": [
    "# ===== 6) 콜백 & 학습 =====\n",
    "# class StopAtAcc(keras.callbacks.Callback):\n",
    "#     def __init__(self, target=EARLY_STOP_AT_ACC): super().__init__(); self.target=target\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         if logs and logs.get(\"accuracy\",0) >= self.target:\n",
    "#             print(f\"\\nReached {self.target*100:.0f}% accuracy — stopping.\")\n",
    "#             self.model.stop_training = True\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[StopAtAcc()]\n",
    "# )\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Prepare sample weights when using soft labels — optional. Set APPLY_SAMPLE_WEIGHT=True to enable.\n",
    "sample_weight = None\n",
    "val_sample_weight = None\n",
    "if LABEL_MODE == 'soft' and APPLY_SAMPLE_WEIGHT:\n",
    "    # down-weight samples whose soft label is near 0.5 (uncertain).\n",
    "    sw = np.abs(y_train - 0.5) * 2.0  # maps distance to 0..1\n",
    "    sw = np.clip(sw, 0.1, 1.0)  # floor to avoid zero weights\n",
    "    sample_weight = sw\n",
    "    vsw = np.abs(y_val - 0.5) * 2.0\n",
    "    val_sample_weight = np.clip(vsw, 0.1, 1.0)\n",
    "    print('Sample weight [train] min/max/mean:', sample_weight.min(), sample_weight.max(), sample_weight.mean())\n",
    "elif LABEL_MODE == 'soft':\n",
    "    print('APPLY_SAMPLE_WEIGHT is False: training will use soft labels directly (0.5 included)')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val, val_sample_weight) if val_sample_weight is not None else (X_val, y_val),\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "    callbacks=[es],\n",
    "    sample_weight=sample_weight,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d48e2252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483us/step\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483us/step\n",
      "Validation results: logloss=0.138375, AUC=0.9887, accuracy=0.9543\n",
      "preds mean/min/max: 0.4953215 2.7592315e-10 1.0\n",
      "soft-label stats: min/max/mean 0.0 1.0 0.5002177\n",
      "Validation results: logloss=0.138375, AUC=0.9887, accuracy=0.9543\n",
      "preds mean/min/max: 0.4953215 2.7592315e-10 1.0\n",
      "soft-label stats: min/max/mean 0.0 1.0 0.5002177\n"
     ]
    }
   ],
   "source": [
    "# ===== EVAL: validation metrics (soft & binary aware) =====\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# predict probabilities\n",
    "preds = model.predict(X_val).flatten()\n",
    "eps = 1e-7\n",
    "preds_clipped = np.clip(preds, eps, 1.0 - eps)\n",
    "# decide which true labels to use\n",
    "if LABEL_MODE == 'soft':\n",
    "    y_true_soft = y_val\n",
    "    y_true_binary = (y_val >= 0.5).astype('float32')\n",
    "elif LABEL_MODE == 'binary':\n",
    "    y_true_binary = y_val\n",
    "    y_true_soft = y_soft_val if 'y_soft_val' in globals() else y_true_binary.astype('float32')\n",
    "else:  # both\n",
    "    y_true_soft = y_soft_val\n",
    "    y_true_binary = y_binary_val\n",
    "# logloss (works with soft targets)\n",
    "logloss = -np.mean(y_true_soft * np.log(preds_clipped) + (1 - y_true_soft) * np.log(1 - preds_clipped))\n",
    "# AUC & Accuracy (binary metrics)\n",
    "auc = roc_auc_score(y_true_binary, preds)\n",
    "acc = np.mean((preds >= 0.5).astype('float32') == y_true_binary)\n",
    "print('Validation results: logloss={:.6f}, AUC={:.4f}, accuracy={:.4f}'.format(logloss, auc, acc))\n",
    "print('preds mean/min/max:', preds.mean(), preds.min(), preds.max())\n",
    "print('soft-label stats: min/max/mean', y_true_soft.min(), y_true_soft.max(), y_true_soft.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "378d8e4c-dce4-4fd9-93de-0349c29e21dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== 7) Plot =====\n",
    "# acc = history.history.get(\"accuracy\", []); val_acc = history.history.get(\"val_accuracy\", [])\n",
    "# plt.figure(); plt.plot(range(1,len(acc)+1), acc, label=\"acc\"); plt.plot(range(1,len(val_acc)+1), val_acc, label=\"val_acc\")\n",
    "# plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Training vs Validation\"); plt.show()\n",
    "\n",
    "acc = history.history.get(\"accuracy\", [])\n",
    "val_acc = history.history.get(\"val_accuracy\", [])\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, label=\"acc\")\n",
    "plt.plot(epochs, val_acc, label=\"val_acc\")\n",
    "plt.ylim(0, 1)                         # 🔁 변경점 4: 축 고정으로 왜곡 방지\n",
    "plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5ba35d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpreyyw3sr/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpreyyw3sr/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpreyyw3sr'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 79), dtype=tf.float32, name='features')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  13337082000: TensorSpec(shape=(1, 79), dtype=tf.float32, name=None)\n",
      "  13337081040: TensorSpec(shape=(1, 79), dtype=tf.float32, name=None)\n",
      "  13337072976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13337083728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13337083344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13337082192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13337081232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13337082960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "TFLite 모델이 저장되었습니다: /Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/phishing_model.tflite\n",
      "피처 정보가 저장되었습니다: /Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/feature_info.json\n",
      "TFLite 모델이 저장되었습니다: /Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/phishing_model.tflite\n",
      "피처 정보가 저장되었습니다: /Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/feature_info.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1763741762.839143 4171299 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1763741762.839155 4171299 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-11-22 01:16:02.839240: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpreyyw3sr\n",
      "2025-11-22 01:16:02.839481: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-11-22 01:16:02.839484: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpreyyw3sr\n",
      "2025-11-22 01:16:02.842179: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-11-22 01:16:02.858717: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/nv/tfw2zdqs7031dn83bb91bwdr0000gn/T/tmpreyyw3sr\n",
      "2025-11-22 01:16:02.863516: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 24276 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# ===== 8) TFLite 변환 =====\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# TFLite 모델 저장\n",
    "# Save outputs into the repo-local phishing/ folder\n",
    "# Use absolute paths for outputs to ensure we write into the repo phishing/ folder\n",
    "tflite_path = \"/Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/phishing_model.tflite\"\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"TFLite 모델이 저장되었습니다: {tflite_path}\")\n",
    "\n",
    "# 피처 정보 저장 (Android에서 사용)\n",
    "import json\n",
    "# Export feature_info with labeling metadata so the app can handle soft labels & uncertainty\n",
    "feature_info = {\n",
    "    \"feature_columns\": numeric_cols,\n",
    "    \"input_shape\": [X_train.shape[1]],\n",
    "    \"normalization_layer\": \"norm_all\",\n",
    "    \"labeling\": LABEL_MODE,  # 'soft'|'binary'|'both'\n",
    "    \"uncertainty_notify_eps\": 0.05  # app should flag predictions in [0.5-eps, 0.5+eps] as low confidence\n",
    "}\n",
    "\n",
    "feature_info_path = \"/Users/seungmin/AndroidStudioProjects/YU_mobile_kotlin/phishing/feature_info.json\"\n",
    "with open(feature_info_path, 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "print(f\"피처 정보가 저장되었습니다: {feature_info_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
